epidemic protocols -> spread new information like an infection

normally every node need as hardcoded list of nodes connected or a service that tells us who are the nodes :
+ centrilized : single point of failure good performance  
+ p2p : try to be as resiliant as possible 

epidemic protocol -> comunicate info to each possible neighbor than each will spread the info to the others

we may send info already known to the others 

stop messagges that i already seen once 

infected node -> has recieved a message and pass to others 
subscetible -> not reacieved a message
removed > infected but want send other messages 

removed nodes -> a way to mitigate the propagation 

keeping track of messages already recieved costly -> we are likely to have reached everyone -> decrese probability of spreading with more time 

1. anti entropy

no removed server , peroidically send messages to each node -> infection does not degrade over time 

we send regardless if no new data has been created 

at each round we always send something

message driven -> when we see a message than we trigger the infection 
its not periodical like before they check , something new ? 

after every round we decrease the probability of infection -> or time to die 

infected and subscetible are on per node basis

find what is missing tÃ¬in the other servers (set difference)

we can use hash trees -> index on a sequence , directly addressable (array rapresentable not linked) -> manage a tree of hash code -> at the last level -> hash code of a message -> than we take a tuple of hash nad hash them until the top -> if the top is equal to ours than all the set is equal

the probability of having a collision needs the be very small 

we can go down until the message

---
share status independently from the failures , with epidemic algo 

---

storing data -> shard each storage unit -> every node has not the full data -> data locality -> local server to handle local data 

less space -> we can comunicate less if we can compute locally 
performance is better for each single shard 

storage capabilities spread on less powerful servers

somputing query distributedly is less affective 

inserting something new its split into different allocation unit > need to decide which server needs to store how much replication 

how to load balance > when we write or read equally distributed load on the network

range base allocation each dist unit some attribute (like birthdate) each server responible for one set of interval fine if uniformly distributed  

cost based -> node that stores the one closest to where the fragment was produced

hash based -> like range based but we impose a uniform distribution with the hash function 

consistent hasing -> unique has for each server -> than an hash code for any data item -> combination of the two to find the server on which we store the data 

ring structure -> if we remove a server all the files before that server we assign them to the next server -> introduce a new server than we split files between two server

each server could know the full ring or pass the element to the next server 

we may have differnt servers -> the others will than assign more data to that server -> simulate the existance of more nodes managed by that server -> ex it can manage 2 server if 2x the normal server

resiliance -> create replicas of the data -> number of replicas 
we can replicate a whole server or only some part of the data 

lower latency -> client connect to the closest , workload can be divided between them -> cost menagement of replicas 

consistency problem someone may have different data -> update needs to be propagate equallu 

master slave replication -> sites that is responsibel of the content and other are readonly

master is single point of failure -< if fails the system will not work

master master both at the same time master and slave for different content 

periodically sync replicas or update notification when something change
no more single point of failure -> part of the data that are mastered by the still alive server it works write

master replication -> every master can handle both writing and reading for all data they sync the change and handle local data 

menaging data is more complex -> differen servers can eveole theyr state separetly -> what if the syncronization is stopped 

when it comes back the link simply the write req gets delayed until it comes back 

if write req depends on data that should be written 
we need to know which is the last message 

hinted handoff -> when server is unavailable -> some other server accept its requests as soon as back online it will sync the changes 

read repair -> read check the majority and use that value to rewrite -> coordinator does this -> distributed election if we dont have a coordinator 

check distributedly if a transaction or write is committed -> confirmation that it git written

commit in 2 phases : 
1. try to commit locally -> request to prepare 
2. coordinator send the real commit and get back the aknowledgment 

only when the final unknowledgment is recieved than we know that the data is really stored 

CAP -> conjecture 
consistency 
availability 
partition tolerance 

2 toghether but not all 3 

strong consistency -> after update returns updated values
eventually consistent -> after some thime the update is passed through