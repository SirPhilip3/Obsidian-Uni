epidemic protocols -> spread new information like an infection

normally every node need as hardcoded list of nodes connected or a service that tells us who are the nodes :
+ centrilized : single point of failure good performance  
+ p2p : try to be as resiliant as possible 

epidemic protocol -> comunicate info to each possible neighbor than each will spread the info to the others

we may send info already known to the others 

stop messagges that i already seen once 

infected node -> has recieved a message and pass to others 
subscetible -> not reacieved a message
removed > infected but want send other messages 

removed nodes -> a way to mitigate the propagation 

keeping track of messages already recieved costly -> we are likely to have reached everyone -> decrese probability of spreading with more time 

1. anti entropy

no removed server , peroidically send messages to each node -> infection does not degrade over time 

we send regardless if no new data has been created 

at each round we always send something

message driven -> when we see a message than we trigger the infection 
its not periodical like before they check , something new ? 

after every round we decrease the probability of infection -> or time to die 

infected and subscetible are on per node basis

find what is missing tÃ¬in the other servers (set difference)

we can use hash trees -> index on a sequence , directly addressable (array rapresentable not linked) -> manage a tree of hash code -> at the last level -> hash code of a message -> than we take a tuple of hash nad hash them until the top -> if the top is equal to ours than all the set is equal

the probability of having a collision needs the be very small 

we can go down until the message

---
share status independently from the failures , with epidemic algo 

---

storing data -> shard each storage unit -> every node has not the full data -> data locality -> local server to handle local data 

less space -> we can comunicate less if we can compute locally 
performance is better for each single shard 

storage capabilities spread on less powerful servers

somputing query distributedly is less affective 

inserting something new its split into different allocation unit > need to decide which server ned