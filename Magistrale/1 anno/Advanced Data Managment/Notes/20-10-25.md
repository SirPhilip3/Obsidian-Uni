cassandra : high ingestion rate as a goal , only api -> access by column name 

not necessary ultra fast access to the data 
1. keeping data in memory very fast but at some point we need to write to the disk

mesure to confront performance between implementation -> educated guess -> count the number of pages that we need to access in the disk -> hypotesis -> no cache 

disk ->
+ append at end of file 
	+ cost = get the last page of a file write it and than write it back to disk
	+ fastest way to write data 
	+ disadvantage : no efficent way of seasrching data by its content

when a memorytable is full is sort it's content and than write it to disk
1 memtable for each  column family

works of if the data is immutable -> ex when use clicked on something
if it changes we need to say that the past event is invalid -> costly to change it we need maybe to change the ordering -> we just say that the previos data is no longer valid 

we have versioning -> sideeffect of other design choices 

changing -> new value that says its the most recent version of the data (timestamps)

deletion using logical deletion -> just flag the record as deleted remains there until compaction > generally more common on disk based infrastructure

we can't add a flag since data is immutable we can't change it -> we need to declered it with adding a new record with same key -> *tombstones* 

still there but invisible to the user , we can do this for a column or a column family -> we can regain data by putting more data with same key

read

we can find data given a key ->
1. in the memtable -> likely the most recent one 
2. in the disk -> we need to look in all the files and retain the one with the highest timestamps -> reliable way to estimate the last timestamp for a specific key

file format 
	one after the other in blocks -> each one a set of data -> several row keys with values -> instead of llokking for the key we can exclude blocks by looking first and last element but we still need to get the page in memory so we add an index that stores the 1st and last element 

index at the beginning or end of a file , end more convenient we can write the index while writing the data blocks -> since it uses many pages -> if its the last one we can store only the blocks that stay in page so we take a trailer at the end as a block that contains metadata about the size and where the index start 

each data block -> seq of key-val pair sorted by key 

key = composed key -> rowkey-columnfamily-columname-timestamp-type-value

type -> what we are doing to key-value pair -> put/delete etcc ??

wulnerable to crashes -> part of the data in the mem table 
writeahead log -> log in which we keep track operations -> keeping trak of what is not write permanently 

in the worst case we lose only the last record

when the data get's flushed the log is removed , at the restart of a system if log not empty it gets written to the memtable

searching 

reorganize data 

reading :
1 page = trailer + 1page index + if found 1 page for data 
for each file best = 2 , worst = 3 pages
so cost to search a key O(n of files)

reduce number of files -> create larger sorted files -> not since the beginning bc we have a memtable that is limited by the mem of the pc 

combine -> on disk sorting algo not as efficent as in memory 

when the fact that we have lots file  the cost makes appealing to reorganize the data 

merge > we have presorted data -> using mergesort

n list -> we chose from the list the one with smallest value
we need a memory page in each file 

how many files to sort ->
+ all the files 
+ based on memory resources that we have 

size of the buffer is effected by the number of files not he size of the files 

we can remove unwanted records -> ex when we find deletion we can remove the data 

bloom filter -> probabilistic filter used to determine set membership -> used to discriminate between elements that we have seen before and one that we have never seen one 

determine if a specific key is in a file or block
errors willing to accept : in general 
exists but not true -> fp -> we go to the data search and we discover that its not there -> acceptable -> provided that it doesnt happend too ofter 
we cant accept false negative -> discard the block without looking at the data > losing a value that it's actually there 

negative always correct
positive could be wrong

bloom filter at the end of the files 

if we see some characteristic of data maybe there if we didn't its not there

array -> 1 for each letter  -> mark the letter that we see the initial of name 
name with a likely to have false positive 
name with q unlikely to gave false positive 

some letters more frequent than others

as characteristics uniform distribution
if we dont have one we use hash function on name so that we have output that is distributed uniformly

non numeric domain 

false positive are created by collision in the hash function 
reduce the prob of collision -> another has function -> another buìit vector that corresponds to another hash function 

prob of false positive = n° of position and n° of element inserted

2 direction for more accurate pred = 
+ increase number of hash function
+ increase size of bit vector 

$m$ = size of the vector
$k$ = number of hash functions
$n$ = number of input keys

1 - 1/m > not present
1- 1-1/m -> error

false positive only if it fails for all the has functions at the same time
we can use to estimate best parameters

