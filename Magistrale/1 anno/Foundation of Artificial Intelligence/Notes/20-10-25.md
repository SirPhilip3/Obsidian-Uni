local search 

local beam search -> keeps track of $k$ states -> if one is goal state stop otherwise select the next $k$ best successors from  the list and repeat 

if we select only the k best it maybe that you search only a local minimum -> varibility of the starting point 

*genetic algorithm* 

in creating successors we recombine more than one solution 
operati directly on the rapresentation of the state -> in string on an alphabet 

fitness function -> generally maximization 
next generation applying some operation : selection crossover mutation

encoding of the problem : operations on strings 

selection -> witch part of the population can go further 

mating pool -> elements that will generate the next generation

invariant string with the same lenght

mutation > low prob that one location is altered 

selection -> only analyzing around good solutions 

asyntotic results -> depends a lot of the implemetnation of each elements

if we have elitism -> best kept on the next population -> ayntotic garantee that the algo will produce the optimal result 

searches in continuos space casting a problem into continuos domain bc we have efficent algorithm to solve that 

relaxation -> probabilistic conversion instead this variable has this value now we have a probability to assign it 

3 types of contraint -> equality concerns , inequality contraints , set contrain

linear programming:
+ simplex method -> at least 1 solution on the vertices 
+ interior point method 

minimize $c^Tx$ 

gradient descent 
f continuos we look at the gradient of f(x) does not depend on coordinate systems  -> vector of the direction of ascend mod = how steep the groth is 

minimum hessian only eigenvalues must be positive , for maximum is negative , sufficeint non negative 

hassian positive scalar matrix -> 1 single step to the solution 
huge difference between igenvalues 

log of the error will decrese leanarly -> slope less and less prononced 

quadratic approxximation -> move to the minimum of the secodn order taylor expansion -> newton method 
x_k - inverse of hessian * 

parallel tangent approach create first and second tangent and search between the initial and finish point 

constrained optimization 
each constraint reduce the dimensionality by 1 and each intersection reduce by one 

directionality of the restriction is dependant on the contraint itself ??? 

\> will contraint the search space 

