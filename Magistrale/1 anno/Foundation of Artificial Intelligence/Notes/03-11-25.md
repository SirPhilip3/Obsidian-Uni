inductive reasoning 

be able to predict the feature given information about the past 

we dont want to memorize the data but predict , generalize the data 
simpler function better 

supervised learning -> in the training set all the instance of a problems are classifed with the correct label 

rule to separate the various domain 

usupervised -> we dont have label on the training 

reinforcment learning -> we have feedback an agent interacting with the environment obtains reward / punishment for theyr action it learn to follow the path of max reward

X -> set of possible imputs Y -> possible outputs 
2 random varaibles with some dependency the joint prob distribution is unknown 

trainig set are set of tuplas sampled from X and Y

the learning algorithm is a mapping between X x Y and the hypotesis H 
and we select a function $f_s$ such that $f_s(x) \approx y$ 

generalization a model that predicts new data -> not all errors are the same 

let V be a loss function that indicates the penalty for a wrong prediction 
the expected / true error : 

we can't know this since we dont know the joint distribution

empirical error -> average over the training set for the loss 

condition for generalization -> regardless of theg joint distr -> the average error converges as the trainig set grows to the true error of the function 

a polynomial perfectly interpolates the empirical data -> always 0 -> expected error not 0  -> 

if i dont have bound on hyotesis space i can laways create new functions that keep the empirical error = 0 -> function must constantly vary 

if f overfits what i observe in after the training set it will not reflect the true error 

control of complexity 
bias vs variance -> tradeoff -> less varaince = less complexity , less bias = less error

the more data i have the more complex solution i will accept 

model assessments 
on the trainig obv the more degrees of freedom the less error > 0 traning set = overfit 

training will always go down but on the test it will reach a point and than increases again -> max generalization

as we increase degrees of freedom the test set worst will increase until degrees = nÂ° of point and than it will decrese again it can surpas for some problems the previous dib or not , the algorithm at n=n 1 single possible fit as increse there are several possible fits -> one can go down -> limiting the number of varaiblesi n the function 

over coefficients of the polynomial > it will increase 
