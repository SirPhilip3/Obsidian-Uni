clustering -> unsupervised learning 

no training set -> we need label in respect to the geometric propieties of the points 

k means algorithm minmization nphard easy for two of the variables but globally hard 

always converge but maybe to local minimum -> for random point decision 

based on euclidian distance -> outlier weight more 

general dissimilarity -> V insead of distance 
in general we find circular clusters -> one of the point will be very close to the centroids -> we use the actual point we dont need to sinthesize a new point 

each point 1 class -> point in the meiddle ??? 

gaussian mixture -> the assignment is not 0,1 but probabilistic closer to the center near to 1 on the border nearer to 0.5

n random variables -> mix them according to a distribution -> how you sample 

add binary indicator that tells us what rand variable of the mixure is used 
normally not known -> we just need to estimate z 

expectation maximization algorithm 

kullback liebber divergence LK
expectation step -> q = p -> maximize the lower bound for a given theta 
posterior p 

---
changing theta chenages the lowerbound of the log likelihood 

bernulli mixture 

we have d independent binary variable for each class 

pixel are correlated in respect to a particular class -> not necessarily true 

generaive model 

---
*mean shift* 

non parametric model that extract classes 

each centere of density will give us a class 

chose initial window size choose initial loc and go towars the baricenter repeat until fix point -> class 

kenels -> circle , square , normal , uniform 

mean shift -> moves with distance given by gradient / density -> low density > greater step otherwise smaller steps 

algo always converges 

will no need any assumption no the shape of the data 

distance is dependant on the data that is rapresented 

for points -> euclidean

instead of distances use hierarchical clustering -> data into trees where lower level more similar 