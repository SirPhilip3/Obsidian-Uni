clustering -> unsupervised learning 

no training set -> we need label in respect to the geometric propieties of the points 

k means algorithm minmization nphard easy for two of the variables but globally hard 

always converge but maybe to local minimum -> for random point decision 

based on euclidian distance -> outlier weight more 

general dissimilarity -> V insead of distance 
in general we find circular clusters -> one of the point will be very close to the centroids -> we use the actual point we dont need to sinthesize a new point 

each point 1 class -> point in the meiddle ??? 

gaussian mixture -> the assignment is not 0,1 but probabilistic closer to the center near to 1 on the border nearer to 0.5

n random variables -> mix them according to a distribution -> how you sample 

add binary indicator that tells us what rand variable of the mixure is used 
normally not known -> we just need to estimate z 

expectation maximization algorithm 

kullback liebber divergence LK
expectation step -> q = p -> maximize the lower bound for a given theta 
posterior p 

---
