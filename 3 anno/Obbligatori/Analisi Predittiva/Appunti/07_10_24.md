---
creation: 2024-10-07T10:31:00
tags:
  - appunti
---
Errore normale 
$$\epsilon\sim N(0,\sigma^2)$$
Se l'ipotesi nulla ha beta_1 = 0 , non possiamo rifiutare che lm è la media 

assenza di evidenza non è evidenza di assenza di effetto , se rifiuto dal t-test semplicemente non c'è abbastanza evidenza che x e y siano spiegati dal linear model

![[Pasted image 20241007104940.png]]
Il linear model non rappresenta i dati ma è evidente che c'è relazione tra x e y

**regressione lineare multipla**

se aggiungiamo un'ulteriore informazione al modello , spieghiamo meglio il modello se diminuiamo l'errore

come stimare l'effetto della seconda covariata ??
$$Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} +\epsilon_i$$
Minimizzo l'*MSE* 

stiamo stimando un piano , questo passerà sempre per $\bar x_1, \bar x_2 , \bar y$ 

per ogni mm di becco , lunghezza della pinna + 0.54 , tenendo contro che il peso è lo stesso

effetti sono addittivi -> i componenti sono additivi , l'iportanza determinata da $\beta_i$ 

descrizione in forma matriciale del modello , *matrice di disegno* -> tutti i valori di x

$$Y= X\beta +\epsilon$$
$X$ -> 
![[designmatrix.excalidraw]]

stima di $s_e$ :
$$s_e^2 = \frac{\sum^{n}_{i=1}(y_i - \hat{y_i})^2}{n-p}$$
dove p il numero di par che stimiamo , ogni parametro perdiamo un grado di libertà del piano

minimizzare somma dei quadrati dei residui : proiezione migliore del piano multi-dimensionale in uno spazio bi-dimenzionale 

stima migliore è se stessa -> voglio ridurre la compessità del modello su meno dimensioni $p$ , quello che non capiamo negli $n-p$ gradi di libertà *residui* -> nell'epsilon

$X$ ha dimensione $n*p$ $H$ ha dim $n*n$ 

$\hat\beta$ che dist ha , var , valore atteso ? -> gaussiane multivariate

se non posso rifiutare $\beta_j = 0$ significa che non è importante considerando gli altri predittori

quelli non significativi possono essere tolti

