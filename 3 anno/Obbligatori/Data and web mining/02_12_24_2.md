---
publish: true
creation: 2024-12-05T15:20:00
---
# Deep Neural Network

>[!important] Definizione
Si dice **Deep Neural Network** le **Artificial Neural Networks** che hanno più di 2 *hidden layer* 

>[!note] 
>Per l'*universal approximation theorem* sappiamo che una **ANN** con un solo *hidden layer* sarebbe sufficente ma aggiungere layer ci permette di velocizzare il *learning*

>[!note] 
>Nel nostro caso una **Deep Neural Network** tutti i *layer* sono *densi*  

## Layers

In **Keras** per definire un *layer* denso dobbiamo dare 3 parametri : 
+ Il numero di *neuroni* del layer
+ L'*activation function*
+ L'*input shape*
>[!example] 
>```python
>model.add(Input(shape=(16,))) # in questo caso si aspetta in input un array contentene 16 numeri , le feature dei dati 
>model.add(Input(shape=(16,16,3))) # si aspetta in input un array 16x16 con 3 canali ( immagine rgb ) 
>```

>[!note] 
>Non è più necessario specificare l'input shape

## Activation Functions

Questa determina l'output di ogni *neurone* 

In `Keras` : 
```python
model.add(layers.Dense(9, activation='sigmoid'))
```
### Sigmoid

![[Pasted image 20241206150758.png]]

La sigmoide è utile per ci permette di ricavare una probabilità compresa tra $[0,1]$ , che è utile per un **classificatore binario** , ma non se vogliamo predire in altri domini ( eg quale sarà la temperatura di domani etcc )

## Loss Function

La **Loss Function** calcola l'errore tra la predizione del network e l'output desiderato 

Una buona **loss function** può essere , nel caso di un task di *regressione* **Mean Squared Error** , nel caso di un task di *predizione* ( *binaria* ) invece si preferisce la **Binary Cross-Entropy** 

In `Keras`
```python
model.compile(loss='mean_squared_error')
```
## Keras Optimizers

Ha il compito di tradurre una *loss* in un aggiornamento dei *weights* 
>[!example] 
>+ Grande *loss* $\to$ Grande update dei pesi
>+ Piccola *loss* $\to$ Piccolo update dei pesi

Tutti gli **optimizer** hanno un parametro per impostare il **learning rate** 

>[!note] 
>Il **learning rate** viene aggiornato adattivamente a seconda 