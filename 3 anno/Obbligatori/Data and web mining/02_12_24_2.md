---
publish: true
creation: 2024-12-05T15:20:00
---
# Deep Neural Network

>[!important] Definizione
Si dice **Deep Neural Network** le **Artificial Neural Networks** che hanno più di 2 *hidden layer* 

>[!note] 
>Per l'*universal approximation theorem* sappiamo che una **ANN** con un solo *hidden layer* sarebbe sufficente ma aggiungere layer ci permette di velocizzare il *learning*

>[!note] 
>Nel nostro caso una **Deep Neural Network** tutti i *layer* sono *densi*  

## Layers

In **Keras** per definire un *layer* denso dobbiamo dare 3 parametri : 
+ Il numero di *neuroni* del layer
+ L'*activation function*
+ L'*input shape*
>[!example] 
>```python
>model.add(Input(shape=(16,))) # in questo caso si aspetta in input un array contentene 16 numeri , le feature dei dati 
>model.add(Input(shape=(16,16,3))) # si aspetta in input un array 16x16 con 3 canali ( immagine rgb ) 
>```

>[!note] 
>Non è più necessario specificare l'input shape

## Activation Functions

Questa determina l'output di ogni *neurone* 

In `Keras` : 
```python
model.add(layers.Dense(9, activation='sigmoid'))
```
### Sigmoid

![[Pasted image 20241206150758.png]]

La sigmoide è utile per ci permette di ricavare una probabilità compresa tra $[0,1]$ , che è utile per un **classificatore binario** , ma non se vogliamo predire in altri domini ( eg quale sarà la temperatura di domani etcc )

### SoftMax

Questa si assicura che l'output sia positivo e che la somma dell'output tra tutti i *neuroni* sia $1$ 
$$
\sigma(z_i) = \frac{e^{z_i}}{\sum^{K}_{k=1} e^{z_k}}
$$
>[!note] 
>**SoftMax** non è una funzione relativa ad un singolo *neurone* ma bensì all'intero layer
![[Pasted image 20241206155240.png]]

In `Keras` :
```python
model.add(layes.Activation('softmax'))
```
## Loss Function

La **Loss Function** calcola l'errore tra la predizione del network e l'output desiderato 

Una buona **loss function** può essere , nel caso di un task di *regressione* **Mean Squared Error** , nel caso di un task di *predizione* ( *binaria* ) invece si preferisce la **Binary Cross-Entropy** 

In `Keras`
```python
model.compile(loss='mean_squared_error')
```

Per task di classificazione non binaria si utilizza la **categorical cross-entropy**
## Keras Optimizers

Ha il compito di tradurre una *loss* in un aggiornamento dei *weights* 
>[!example] 
>+ Grande *loss* $\to$ Grande update dei pesi
>+ Piccola *loss* $\to$ Piccolo update dei pesi

Tutti gli **optimizer** hanno un parametro per impostare il **learning rate** 

>[!note] 
>Il **learning rate** viene aggiornato adattivamente a seconda se ci stiamo o no avvicinando al minimo 
>
>Generalmente iniziamo con un grande *learning rate* e andiamo via via diminuendolo 

`Keras` fornisce i seguenti *optimizer* : 
+ `sgd` : bassto sul *gradient descent*
+ `rmsprop` : mantiene un *learning rate* adattivo per ogni parametro
+ `adam` : un miglioramento dei precedenti 

## Example

### Digit Recognition

Visto che abbiamo 10 numeri da riconoscere il layer di *output* avrà 10 *neuroni* , ognuno rappresenterà la probabilità che l'input sia $i$-esima cifra , la *classificazione* sarà basata sul neurone che produce l'output maggiore ( maggiore probabilità )

Visto che ognuno dei 10 *neuroni* rappresenta la probabilità che un certo input sia una certa cifra volgimamo che queste sommino ad $1$ 

Per fare ciò utilizziamo l'*activation function* **SoftMax**

#todo

Visto che in output la rete neurale ritorna un vettore di dimensione 10 , per poterlo confrontare attraverso la **loss function** dee essere nello stesso formato dell'output della rete

>[!example] 
>Se la **ANN** da in output : 
>`[ 0.1, 0.1, 0.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.1]`
>Questo significa che prevede che la cifra che più probabilemnte era rappresentata nell'input era l'`8` 
>Se l'output desiderato invece era `9` , vogliamo rappresentarlo nello stesso formato dell'output della rete:
>`[ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]`

---

>[!example] Example network

```python
# linearize images
train_images = train_images.reshape( (num_images, img_x * img_y) )
train_images = train_images.astype("float64") / 255.0
# one-hot-encoding of labels
train_labels = to_categorical(train_labels)

# initialize the network
model = Sequential()

# specify input shape (optional)
model.add( Input(shape=(img_x*img_y,)) )

# add nodes to the network
model.add( Dense(15) ) # neurons in the first hidden layer
model.add( Activation("sigmoid") )

model.add( Dense(10) )
model.add( Activation("softmax") )

# finalize the network
model.compile( optimizer="rmsprop", #optimizers.Adam(lr=.01),
               loss='categorical_crossentropy',
               metrics=['acc'] )

# train the network
hist = model.fit( x=train_images, # training examples
                  y=train_labels, # desired output
                  epochs=10,      # number of training epochs 
                  validation_split = 0.2, # 20% randomly sampled
                  verbose=1)
```

![[Pasted image 20241206161013.png]]

>[!warning] 
>Questa rete è sensibile a traslazioni , rotazioni e scaling delle immagini in input poichè non impara nulla riguardante i pattern di una determinata immagine solo le posizioni dei pixel
### Regression Task

Vogliamo approssimare una funzione sconosciuta $f$ di cui conosciamo un set di mappings da una variabile di input $X$ ad una , *continua* , di output $y$  

>[!example] 
>![[Pasted image 20241207172949.png]]

 
Visto che l'output della *neural network* può essere un valore compreso tra $[-\infty,\infty]$ , non possiamo usare *activation function* come la *sogmoide* o *softmax* poichè ritorano un valore compreso tra $[0,1]$ 

La soluzione è semplicemente non applicare alcuna *activation function*

>[!note] 
>In `Keras` basta non specificare l'*activation function*

Come *loss function* per la *regression* generalmente si utilizza il **Mean Squared Error** 

>[!note]- 
>`Keras` fornisce diversi tipi di **MSE** :
>+ `MSE`
>+ `MAE` : `mean_absolute_error` : $\frac{1}{n} \sum^{n}_{i=1} |x_i - x|$
>+ `MAPE` : `mean_absolute_percentage_error` :  $\frac{1}{n} \sum^{n}_{i=1} |\frac{|A_i-F_i|}{A_i}|$
>+ `MSLE` : `mean_squared_logarithmic_error` $\frac{1}{n} \sum^{n}_{i=1}(\log(1-\hat{y_i})-\log(1+y_i))^2$ 

>[!example] 
>Example network
>
>```python
>model = Sequential()
>
># add nodes to the network
>model.add( Input(shape=(1,)) )
>model.add( Dense(8) )
>model.add( Activation("sigmoid") ) 
>
>model.add( Dense(32) )
>model.add( Activation("sigmoid") )
>
>model.add( Dense(1) )
># No Activation
>
># finalize the network
>model.compile( optimizer="rmsprop",
>               loss='mean_squared_error',
>               metrics=['mse', 'mae'] )
>```
![[Pasted image 20241207175102.png]]

# Convulational Neural Networks

Per fare in modo che una *ANN* per riconoscere immagini impari dei pattern invece che la posizione dei pixel utilizziamo i **convulational layer**

## Convulational Filters

I **filtri convolutivi** si applicano all'immagine di input e ne producono un'altra in output

Ogni filtro presente in un layer cerca , ossia viene applicato ad ogni pixel dell'immagine in input ( l'output su un singolo pixel viende detto **response** ) , un pattern all'interno dell'immagine  

La dimensionalità dell'immagine in output sarà :
+ La dimensione in pixel verrà ridotta di qualche pixel poichè i filtri vengono applicati non sovrapponendosi ai bordi dell'immagine
+ Per ogni filtro che applichiamo si aggiunge un *canale* all'immagine finale

![[Pasted image 20241209085843.png]]

>[!note] 
>I pesi all'interno del filtro vengono imparati dalla rete 

>[!warning] 
>L'immagine risultante è ancora sensibile agli spostamenti ma è diventata robusta rispetto alla traslazione

Se applichiamo filtri all'immagine risultante i filtri potrenno riconoscere pattern più complessi ( la combinazione dei pattern imparati in precedenza )

![[Pasted image 20241209090412.png]]

>[!note] 
>Per vedere cosa hanno imparato i filtri si genera un immagine che massimizzi il filtro

In `Keras` :
```python
model.add( Conv2D(64, (3, 3)) )
```
## Pooling Layers

Essendo che i **convulational filter** generano sempre una **response** questà sarà molto ridondante poichè un pattern è presente solo quando massimizza il valore del filtro e visto che lo spostiamo un pixel alla volta se sarà massimizzato in un pixel è molto probabile che non lo sia nei suoi pixel vicini

Per questo utilizziamo il **max-pooling** dove genereremo un output per ogni finestra che non si sovrappone 

>[!example] 
>![[Pasted image 20241209091600.png]]
>
>In questo caso l'output sarà un'immagine che la sua dimensione dimezzata ma che mantiene la presenza dei pattern 

In questo modo i layer sucessivi di **convolution** e **pooling** avranno solo metà dei dati in entrata

>[!note] 
>**Pooling layers** non hanno pesi da imparare

In `Keras` :
```python
model.add( MaxPooling2D((2, 2)) )
```

## Last Classification Layers

Nell'esempio della classificazione di cifre , la rete deve prima o poi convergere a 10 stime di probabilità 

L'output dell'ultimo *max-pooling* viene linearizzato da una matrice tridimensionale $width \times height \times \#channels$  ad un vettore unidimensionale da un layer chiamato `Flatten` che non ha nessun peso da imparare

Collegiamo poi questo layer ai 10 neuroni finali attraverso un layer *denso* con attivazione **softmax** 

```python
...
model.add( MaxPooling2D((2, 2)) )

model.add( Flatten() )
model.add( Dense(64) )
model.add( Activation("relu") )

model.add( Dense(10) )
model.add( Activation("softmax") )
```

## Examples

Generalmente si utilizzano più ripetizioni dello stack **convulational layer** + **pooling layer** in modo da cercare pattern sempre più ad alto livello
### 2D Convolution

![[Pasted image 20241207181615.png]]