---
creation: 2024-10-07T12:19:00
---
# Gain

## Information Gain

>[!note] 
>Primo algoritmo ad implementarlo : **ID3** o **Iterative Dichotomiser**

$$Error(D)= Info(D)=-\sum_{i} p_i \log_2(p_i)$$
Questo valore rappresenta l'**entropia** ( una misura della causalità ( più è alta e più c'e rumore/errore nel dataset ) ) dove : 
+ $p_i$ rappresenta la probabilità / frequenza della *label* $i$ 

L'*entropia* ha come valori :
+ *massimo* : $\log m$ , dove $m$ rappresenta il numero di classi , questo rappresenta la condizione in cui tutte le classi sono equamente distribuite nel dataset  
+ *minimo* : 0 , questo rappresenta la condizione in cui tutti i *record* sono di una singola classe ( siamo in una foglia *pura* ) 

In questo modo diamo priorità alle foglie *pure* 

>[!example] 
>Supponiamo che $|D|$ abbia 400 *istanze* in classe 0 e 400 in classe 1
>
>Inizialmente avremo quindi che le classi sono distribuite equamente all'interno di $D$ , ci aspettiamo quindi che l'entropia si $1$  ( questo perchè $\log_2 (2)=1$ ) 
>
>$$Error(D)=-1/2\log(1/2)-1/2\log(1/2) = \log(2)=1$$
>>[!note] 
>>Visto che le *istanze* sono equamente divise avremo $p_1 = p_2 = 1/2$
>
>Supponiamo ora di svolgere il seguente split $A$ : $D_L = (300,100)$ , $D_R = (100,300)$ 
>
>Calcoliamo il *gain* : 
>$$Gain(A|D) = 1 - 400/800 \cdot \left(-\frac{3}{4}\log \left(\frac{3}{4}\right) - \frac{1}{4}\log \left(\frac{1}{4}\right)\right)- 400/800 \cdot \left(-\frac{1}{4}\log \left(\frac{1}{4}\right) - \frac{3}{4}\log \left(\frac{3}{4}\right)\right)\approx 0.19$$
>
>Supponiamo ora lo split $B$ : $D_L = (200,400)$ , $D_R = (200,0)$ 
>
>Calcoliamo il *gain* : 
>$$Gain(B|D) = 1 - 600/800 \cdot \left(-\frac{1}{3}\log \left(\frac{1}{3}\right) - \frac{2}{3}\log \left(\frac{2}{3}\right)\right)- 200/800 \cdot (-1\log (1) - 0\log (0))\approx 0.31$$
>
>Essendo il *gain* migliore il più alto sciegliamo lo split $B$

## Gain Ratio

>[!note] 
>Primo algoritmo ad implementarlo : **C4.5**

L'*information gain* favorisce alberi $k$-ari con molti figli poichè c'è maggiore probabilità che queste siano pure ( non è necessariamente il miglior modo per splittare )

Dobbiamo quindi aggiugnere una penalizzazione per il numero di figli :
$$SplitInfo(D) = - \sum^{k}_{i=1}\frac{|D_i|}{|D|} \log \left(\frac{|D_i|}{|D|}\right)$$
Questo è analogo all'*information gain* ma invece di misurare la purezza di uno split misuriamo la distribuzione delle istanze nei vari sottogruppi 

Il *gain ratio* risulta essere : 
$$GainRatio(D)= \frac{InformationGain(D)}{SplitInfo(D)}$$
## GINI Index

# Stopping Criteria

# Regression