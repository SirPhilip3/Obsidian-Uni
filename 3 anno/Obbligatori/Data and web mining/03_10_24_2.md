---
creation: 2024-10-07T12:19:00
---
# Gain

## Information Gain

>[!note] 
>Primo algoritmo ad implementarlo : **ID3** o **Iterative Dichotomiser**

$$Error(D)= Info(D)=-\sum_{i} p_i \log_2(p_i)$$
Questo valore rappresenta l'**entropia** ( una misura della causalità ( più è alta e più c'e rumore/errore nel dataset ) ) dove : 
+ $p_i$ rappresenta la probabilità / frequenza della *label* $i$ 

L'*entropia* ha come valori :
+ *massimo* : $\log m$ , dove $m$ rappresenta il numero di classi , questo rappresenta la condizione in cui tutte le classi sono equamente distribuite nel dataset  
+ *minimo* : 0 , questo rappresenta la condizione in cui tutti i *record* sono di una singola classe ( siamo in una foglia *pura* ) 

In questo modo diamo priorità alle foglie *pure* 

>[!example] 
>Supponiamo che $|D|$ abbia 400 *istanze* in classe 0 e 400 in classe 1
>
>Inizialmente avremo quindi che le classi sono distribuite equamente all'interno di $D$ , ci aspettiamo quindi che l'entropia si $1$  ( questo perchè $\log_2 (2)=1$ ) 
>
>$$Error(D)=-1/2\log(1/2)-1/2\log(1/2) = \log(2)=1$$
>>[!note] 
>>Visto che le *istanze* sono equamente divise avremo $p_1 = p_2 = 1/2$
>
>Supponiamo ora di svolgere il seguente split $A$ : $D_L = (300,100)$ , $D_R = (100,300)$ 
>
>Calcoliamo il *gain* : 
>$$Gain(A|D) = 1 - 400/800 \cdot \left(-\frac{3}{4}\log \left(\frac{3}{4}\right) - \frac{1}{4}\log \left(\frac{1}{4}\right)\right)- 400/800 \cdot \left(-\frac{1}{4}\log \left(\frac{1}{4}\right) - \frac{3}{4}\log \left(\frac{3}{4}\right)\right)\approx 0.19$$
>
>Supponiamo ora lo split $B$ : $D_L = (200,400)$ , $D_R = (200,0)$ 
>
>Calcoliamo il *gain* : 
>$$Gain(B|D) = 1 - 400/800 \cdot \left(-\frac{3}{4}\log \left(\frac{3}{4}\right) - \frac{1}{4}\log \left(\frac{1}{4}\right)\right)- 400/800 \cdot \left(-\frac{1}{4}\log \left(\frac{1}{4}\right) - \frac{3}{4}\log \left(\frac{3}{4}\right)\right)\approx 0.19$$




## Gain Ratio

## GINI Index

# Stopping Criteria

# Regression