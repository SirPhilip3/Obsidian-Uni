---
creation: 2024-10-07T12:19:00
publish: true
---
# Gain

## Information Gain

>[!note] 
>Primo algoritmo ad implementarlo : **ID3** o **Iterative Dichotomiser**

$$Error(D)= Info(D)=-\sum_{i} p_i \log_2(p_i)$$
Questo valore rappresenta l'**entropia** ( una misura della causalità ( più è alta e più c'e rumore/errore nel dataset ) ) dove : 
+ $p_i$ rappresenta la probabilità / frequenza della *label* $i$ 

L'*entropia* ha come valori :
+ *massimo* : $\log m$ , dove $m$ rappresenta il numero di classi , questo rappresenta la condizione in cui tutte le classi sono equamente distribuite nel dataset  
+ *minimo* : 0 , questo rappresenta la condizione in cui tutti i *record* sono di una singola classe ( siamo in una foglia *pura* ) 

In questo modo diamo priorità alle foglie *pure* 

>[!example] 
>Supponiamo che $|D|$ abbia 400 *istanze* in classe 0 e 400 in classe 1
>
>Inizialmente avremo quindi che le classi sono distribuite equamente all'interno di $D$ , ci aspettiamo quindi che l'entropia si $1$  ( questo perchè $\log_2 (2)=1$ ) 
>
>$$Error(D)=-1/2\log(1/2)-1/2\log(1/2) = \log(2)=1$$
>>[!note] 
>>Visto che le *istanze* sono equamente divise avremo $p_1 = p_2 = 1/2$
>
>Supponiamo ora di svolgere il seguente split $A$ : $D_L = (300,100)$ , $D_R = (100,300)$ 
>
>Calcoliamo il *gain* : 
>$$Gain(A|D) = 1 - 400/800 \cdot \left(-\frac{3}{4}\log \left(\frac{3}{4}\right) - \frac{1}{4}\log \left(\frac{1}{4}\right)\right)- 400/800 \cdot \left(-\frac{1}{4}\log \left(\frac{1}{4}\right) - \frac{3}{4}\log \left(\frac{3}{4}\right)\right)\approx 0.19$$
>
>Supponiamo ora lo split $B$ : $D_L = (200,400)$ , $D_R = (200,0)$ 
>
>Calcoliamo il *gain* : 
>$$Gain(B|D) = 1 - 600/800 \cdot \left(-\frac{1}{3}\log \left(\frac{1}{3}\right) - \frac{2}{3}\log \left(\frac{2}{3}\right)\right)- 200/800 \cdot (-1\log (1) - 0\log (0))\approx 0.31$$
>
>Essendo il *gain* migliore il più alto sciegliamo lo split $B$

## Gain Ratio

>[!note] 
>Primo algoritmo ad implementarlo : **C4.5**

L'*information gain* favorisce alberi $k$-ari con molti figli poichè c'è maggiore probabilità che queste siano pure ( non è necessariamente il miglior modo per splittare )

Dobbiamo quindi aggiugnere una penalizzazione per il numero di figli :
$$SplitInfo(D) = - \sum^{k}_{i=1}\frac{|D_i|}{|D|} \log \left(\frac{|D_i|}{|D|}\right)$$
Questo è analogo all'*information gain* ma invece di misurare la purezza di uno split misuriamo la distribuzione delle istanze nei vari sottogruppi 

Il *gain ratio* risulta essere : 
$$GainRatio(D)= \frac{InformationGain(D)}{SplitInfo(D)}$$
## GINI Index

>[!note] 
>Primo algoritmo ad implementarlo : **CART** - *Classification and Regression Trees*

Il **GINI Index** rappresenta una misura di *dispersione* ( misura ossia la probabiltà di trovare istanze di diverse classi in un dataset )
$$Error(D)=Gini(D)=1-\sum_i p_i^2$$
Dove $p_i$ è la proporzione di elementi che appartengono alla classe $i$

L'*indice* avrà come valore : 
+ *Massimo* : $(1-1/m)$ dove $m$ sono il numero delle classi , rappresenta la situazione in cui tutte le *istanze* sono distribuite equamente tra tutte le classi ( ci penalizza )
+ *Minimo* : $0$ indica la massima *purezza* , ossia tutte le istanze appartengono alla stessa classe ( nodo puro ) ( preferisce )

>[!example] 
>Supponiamo di avere un *dataset* $D$ con 400 istanze di classe 0 e 400 istanze di classe 0 , $D=(400,400)$ 
>$$Error(D) = 1 - (1/2)^2 - (1/2)^2=0.5$$
>Che rispecchia anche il valore massimo pocihè le due istanze sono distribuite equamente : $1-1/2 = 0.5$
>
>Supponiamo uno *split* $A=(f_1,t_1)$ che produce $D_L=(300,100)$ e $D_R=(100,300)$ : 
>$$Gain(A|D) = 0.5 - 400/800 * (1-(3/4)^2 -(1/4)^2)-400/800*(1-(1/4)^2-(3/4)^2)=0.125$$
>
>Supponiamo uno *split* $B=(f_2,t_2)$ che produce $D_L=(200,400)$ e $D_R=(200,0)$ : 
>$$Gain(B|D) = 0.5 -600/800 * (1-(1/3)^2-(2/3)^2)-200/800*(1-1^2-0^2) \approx 0.167$$
>
>Vediamo che preferiamo il *gain* maggiore dello split $B$
# Stopping Criteria

Per evitare l'*overfitting* utilizziamo dei critieri di stop per fermare la specializzazione dell'*albero di decisione* : 
## Max leaf nodes

In `sklearn` possiamo definire il numero massimo di nodi *foglia* all'interno del nostro *albero di decisione* nel seguente modo : 

```python
dt = DecisionTreeClassifier(max_leaf_nodes = n)
```

Per trovare la migliore $n$ possiamo scrivere  :

```python
for max_leaves in range(2,50):
    # train and predict
    dt = DecisionTreeClassifier(max_leaf_nodes=max_leaves)
    dt.fit(X_train,y_train)
    # compute Accuracy
    train_ac=accuracy_score(y_true=y_train,y_pred=dt.predict(X_train))
    test_ac=accuracy_score(y_true=y_test,y_pred = dt.predict(X_test))
```

## Maximum Depth

Possiamo definire anche una *profondità massima* , questa renderà l'*albero di decisione* risultante *bilanciato*

In `sklearn` possiamo scrivere

```python
dt = DecisionTreeClassifier(max_depth = n)
```

Per trovare la migliore $n$ possiamo scrivere  :

```python
for max_depth in range(2,50):
    # train and predict
    dt = DecisionTreeClassifier(max_depth=max_depth)
    dt.fit(X_train,y_train)
    # compute Accuracy
    train_ac=accuracy_score(y_true=y_train,y_pred=dt.predict(X_train))
    test_ac=accuracy_score(y_true=y_test,y_pred = dt.predict(X_test))
```
## Algorithm with Costrains

```pseudo
	\begin{algorithm}
	\caption{BuildTree($D$)}
	\begin{algorithmic}
	\State $Tree \leftarrow \emptyset$
	\State $(f \le t) \leftarrow \text{best split of D}$
	\State $Gain \leftarrow \text{goodness of the split} (f\le t)$
	\State $Queue \leftarrow (gain , (f\le t), D)$
	\While{$Queue \neq \emptyset \text{ and no other stopping criteria is met :}$}
		\State $( gain, (f \leq t), D^*) \gets Queue.\text{pop\_max()}$
	    \State Add node $(f\le t)$ to $Tree$ at the leaf correspoing to $D^*$
	    \State § Left Partition
	    \State $D_L \leftarrow \{x \in D | x_f \le t\}$
	    \State $(f\le t) \leftarrow$ best split of $D_L$
	    \State $Gain \leftarrow$ goodness of the split $(f\le t)$
	    \State $Queue$.push($(gain,(f \le t),D_L)$)
	    \State § Right Partition
	    \State $D_R \leftarrow \{x \in D | x_f \le t\}$
	    \State $(f\le t) \leftarrow$ best split of $D_R$
	    \State $Gain \leftarrow$ goodness of the split $(f\le t)$
	    \State $Queue$.push($(gain,(f \le t),D_R)$)
    \EndWhile
    \Return $Tree$
	\end{algorithmic}
	\end{algorithm}
```

>[!note] 
>Vi sono anche altri parametri utilizzati per fare la *messa a punto* dell'albero :
>+ `min_samples_split` : Il minimo numero di *campioni* richiesti per fare lo *split* di un nodo interno
>+ `min_samples_leaf` : Il minimo numero di *campioni* richiesti per fare lo *split* di un nodo foglia 
>+ `min_inpurity_decrease` : Il minimo *gain* per consentire lo *split*
>+ `min_impurity_split` : Il minimo *error* per consentire lo *split*

# Regression

I **Decision Tree** possono anche essere utilizzati in problemi di *regressione* 

Utilizziamo come misura di errore l'**MSE** : 
$$Error(tree, {\cal D}) = MSE(tree, {\cal D}) =  \frac{1}{|{\cal D}|} \sum\limits_{(x,y)\in {\cal D}} (tree(x)- y)^2$$
## Leaf Node

Nel *leaf node* mettiamo la predizione che *minimizza* l'**MSE** :
$$
\mu = \arg\min MSE(\mu, {\cal D})\quad \Rightarrow\quad \mu =  \frac{1}{|{\cal D}|} \sum\limits_{(x,y)\in {\cal D}} y
$$
( ossia $\mu$ deve essere il valore medio delle *labels* in $\mathcal{D}$ )

Dato quindi le predizioni delle *foglie* possiamo calcolare l'errore totale del dataset in :
$$
Error({\cal D}) =  \frac{1}{|{\cal D}|} \sum\limits_{(x,y)\in {\cal D}} (\mu - y)^2
$$
## Internal Node

Dato un paio $f\le t$ determiniamo la qualità dello *split* a seconda della diminuzione dell'*errore* prima e dopo lo split del nodo : 
$$
Gain(f,t~|~{\cal D}) = Error({\cal D}) - \frac{|{\cal D}_L|}{|{\cal D}|} Error({\cal D}_L) - \frac{|{\cal D}_R|}{|{\cal D}|} Error({\cal D}_R)
$$