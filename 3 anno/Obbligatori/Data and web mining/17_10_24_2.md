---
creation: 2024-10-29T10:48:00
publish: true
---
# Model Selection

## k-fold Cross-validation

>[!warning] 
>La partizione in **Training** , **Validation** e **Test** potrebbe portare ad avere un dataset di **Validation** "facile" o "difficile" , questo può portarmi a sovrastimare o sottostimare la vera accuracy del modello

Possiamo ridurre questo effetto dividendo il *training set* in $k$ parti , ogni parte verrà usata una volta per determinare l'*accuracy* del *validation set* 

>[!example] 
>Con $k=5$
![[Pasted image 20241030104219.png]]

In `sklearn` questo viene implementato in `cross_val_score`

```python
# ritorna un array di accuracies , una per ogni divisione del training
scores = cross_val_score(model, X_train_80, y_train_80, 
                         cv=5, scoring='accuracy', 
                         verbose = 0)
```
## Stratified Sampling

Invece di sciegliere i dati , scegliamo le entità in modo che queste la distribuzione delle classi abbiano proporzione simile tra i vari dataset di test / validation

>[!note] 
>Questo metodo viene utilizzato nel `cross_val_score` di `sklearn`

## Automatic Parameter tuning

`sklearn` implementa la funzione `GridSeachCV` , questo dato un modello , una lista di parametri con relativi valori da testare , attraverso il `cross_val` seleziona i parametri migliori

>[!example] 

```python
model = SVC()
parameters = { 'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}
tuned_model = GridSearchCV(model, parameters, cv=5, verbose=0)
tuned_model.fit(X_train_80, y_train_80)
```
# Ensamble Methods

Invece di utilizzare un solo classificatore per i nostri dati , mettiamo assieme vari predittori in modo da migliorare l'*accuracy*

>[!note] 
>Generalmente prendiamo un insieme di **weak learners** , ossia modelli con un bassa *accuracy* ( generalmente leggeremente superiore a random ) e relativamente semplici 

>[!example] 
>Prendiamo 3 classificatori con *accuracy* del $66\%$ ( il loro errore sarà quindi $33\%$ )
>Prediciamo quello che predice la maggior parte dei predittori , la probabbiltià che la maggioranza si sbagliata sarà quindi : 
>$$3\cdot (1/3\cdot1/3 \cdot2/3 )+(1/3\cdot1/3 \cdot1/3 ) = 26\%$$
>
>La loro combianzione ha quindi *accuracy* di $74\%$ 

>[!warning] 
>Assumiamo che i *classificatori* siano indipendenti l'uno dall'altro , se suddividiamo il dataset iniziale per ogni classificatore , le suddivisioni saranno troppo piccole per essere rappresentative dei dati iniziali

Per risolvere questi problema utilizziamo **Sampling with Replacement** ( ossia creo $n$ dataset della stessa dimensione di quello originale ma ogni istanza che scelgo viene reintrodotta nel dataset iniziale ) anche detto **Bootstrap Sampling**

>[!note] 
>In questo modo nei dataset creati possiamo avere dati duplicati 

La predizione finale verrà presa a seconda della : 
+ **Maggioranza** per task di classificazione 
+ **Media** per task di regressione 
## Bagging ( Bootstrap Aggregation )

I **bootstrap samples** sono tutti differenti , infatti la probabilità che un *istanza* faccia parte di un *sample* è : 
$$1-(1-1/N)^N$$ Con $N$ , la dimensione del dataset , molto gande questa converge a $1-1/e \approx 0.632$ 

```pseudo
	\begin{algorithm}
	\caption{Bagging}
	\begin{algorithmic}
	\For{$i = 1,\dots,k$}
		\State Crea un bootstrap sample $D_i$ da $D$
		\State Costruisci un modello $M_i$ su $D_i$
    \EndFor
    \State La predizione per un istanza $x$ è determinata per Maggioranza (in caso di classificazione) o Media (in caso di regressione)
	\end{algorithmic}
	\end{algorithm}
```

>[!note] 
>Gli alberi generati sono molto simili tra di loro , per questo l'*accuracy* finale ha difficoltà a raggiungere il 100%

>[!important] 
>Il *Bagging* aiuta a diminuire la varianza dei dati ( quando la predizione è dispersa dalla media )
## Boosting 

Cerchiamo di correggere le *istanze* per cui la predizione non è *corretta*  

Ogni *istanza* ha un peso associato , *istanze* che vengono classificate in modo scorretto hanno un peso maggiore e vengono quindi selezionate con maggiore probabilità per la prossima iterazione 

Inizialmente tutte le istanze ( $x_j$ ) hanno $w_j = 1/N$ dove $N$ è la dimensione del dataset 
### Weight calculation

Dopo aver allenato un modello $M_i$ con un dataset $D_i$ calcoliamo il suo errore : 
$$error(M_i)= \frac{1}{N} \cdot \sum_{(x_j,y_j)} w_j \cdot 1 [M_i(x_j) \neq y_j]$$
Dove $1[condition] = 1$ solo se $condition$ è `true` , 0 altrimenti

$error(M_i)$ è grande quindi quando le predizioni di $M_i$ *non* sono *corrette* per istanze con grandi pesi

Ogni **learner** viene pesato a seconda della sua *accuracy* 
$$\alpha_i = \frac{1}{2} \log \frac{1-error(M_i)}{error(M_i)}$$
>[!note] 
>Per modelli il cui errore è uguale a $0.5$ , il suo peso viene mappato a $0$ 
>I modelli che hanno un *errore* maggiore di $0.5$ vengono **eliminati**

$\alpha_i$ viene usato per aggiornare i *pesi* delle singole istanze : 
+ Per istanze predette *correttamente* avremo che $w_j = w_j \cdot e^{-\alpha_i}$
+ Per istanze predette *non correttamente* avremo che $w_j = w_j \cdot e^{\alpha_i}$

>[!note] 
>Normalizzazione dei pesi : $$w_j = \frac{w_j}{\sum_k w_k}$$

>[!example] 
>```python
>dt = DecisionTreeClassifier(max_leaf_nodes=8)
>adaboost = AdaBoostClassifier(dt, n_estimators=10)
>
>adaboost.fit(X,y)
>accuracy = accuracy_score(y_true=y, y_pred=adaboost.predict(X))
>```

>[!warning] 
>Il *boosting* è soggetto a *overfitting* , questo perchè essendo che cerca di correggere le istanze missclassificate cercherà di correggere anche gli *outlier* e il *noise*

La predizione finale viene presa prendendo una media pesata delle predizioni fatte da ogni classificatore 

>[!important] 
>Il *Boosting* aiuta a diminuire il **Bias** ( quando la predizione è distante dalla media )