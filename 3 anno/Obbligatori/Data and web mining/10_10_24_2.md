---
creation: 2024-10-28T15:27:00
---
# Linear regression

La *regressione lineare* ci permette di individuare una retta che indichi la tendenza che prendono i dati 

$$y = b_0 + b_1x+\dots + b_nx$$

Tipicamente questa retta viene minimizzata rispetto all'**MSE** ( **Mean Square Error** ) o **RMSE** ( **Root Mean Square Error** ) : 
$$RMSE(y\_true, y\_pred ) = \sqrt{\frac{\sum_i (y\_true[i]-y\_pred[i])^2}{\#y\_true}}$$
```python
train_error=root_mean_squared_error(y_true=y_train,y_pred=y_pred[:train_size])
test_error=root_mean_squared_error(y_true=y_test,y_pred=y_pred[train_size:])
```

>[!note] 
>Il modello *lineare* potrebbe non essere sufficente per rappresentare efficamentente i dati , possiamo aumentare il grado delle *features* creando un modello **polinomiale**
>$$y = b_0 + b_1x+b_2x^2$$
>
>```python
>poly = PolynomialFeatures(2, include_bias=False)
>
>poly.fit(X_train) # just computes the number of additional features
>
>poly.transform(X_train)
>```

>[!warning] 
>Se aumentiamo troppo il grado del polinomio possiamo arrivare a fare un **overfitting** sei dati 
# Logistic Regression

Vogliamo trasformare una *linear regression* che ritorna valori continui $[-\infty , \infty]$ in una funzione che ritorna un interallo compreso tra $[0,1]$ che verrà interpretato come la probabilità che un *istanza* appartenga ad una classe $0$ invece che ad una classe $1$  

Per fare questo utilizziamo una *sigmoide* ( *logistica* ) : 
$$P(y=1|x)=sig(z)=\frac{1}{1-e^{-z}}$$
dove $z$ è la *regressione lineare* : $z = X^T\upbeta + \epsilon$
## SVM ( Support Vector Machines )

Le due classi vengono separate da una *decision boundary* , vogliamo trovare la migliore *decision boundary* , ossia quella retta che divide meglio le due classi

![[Pasted image 20241029114506.png]]

Questa viene scelta come quella retta che lascia il maggior **margine** ( distanza tra le due istanze più vicine delle due classi ) tra le due classi 

>[!note] 
>Più piccollo è il **margine** maggiore sarà il rischio di missclassificare delle istanze

>[!note] 
>Quelle istanze che determinano il **margine** vengono dette **support vector**

**Soft margin**

Il metodo precedente non funziona per classi *non separabili linearmente* , oltre a massimizzare il **margine** quindi aggiungiamo una costante $C$ che moltiplica l'errore fatto 

Un valore maggiore di $C$ implica un margine che da più importanza a minimizzare l'errore del training rispetto a massimizzare il **margine**

### Non Linearly-separable problems

Trasformiamo i dati dal loro spazio originale ad uno spazio a più alta dimensionalità , trovo il **margine** massimo

![[Pasted image 20241029120427.png]]