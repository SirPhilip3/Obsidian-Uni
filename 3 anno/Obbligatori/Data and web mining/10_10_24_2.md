---
creation: 2024-10-28T15:27:00
---
# Linear regression

La *regressione lineare* ci permette di individuare una retta che indichi la tendenza che prendono i dati 

$$y = b_0 + b_1x+\dots + b_nx$$

Tipicamente questa retta viene minimizzata rispetto all'**MSE** ( **Mean Square Error** ) o **RMSE** ( **Root Mean Square Error** ) : 
$$RMSE(y\_true, y\_pred ) = \sqrt{\frac{\sum_i (y\_true[i]-y\_pred[i])^2}{\#y\_true}}$$
```python
train_error=root_mean_squared_error(y_true=y_train,y_pred=y_pred[:train_size])
test_error=root_mean_squared_error(y_true=y_test,y_pred=y_pred[train_size:])
```

>[!note] 
>Il modello *lineare* potrebbe non essere sufficente per rappresentare efficamentente i dati , possiamo aumentare il grado delle *features* creando un modello **polinomiale**
>$$y = b_0 + b_1x+b_2x^2$$
>
>```python
>poly = PolynomialFeatures(2, include_bias=False)
>
>poly.fit(X_train) # just computes the number of additional features
>
>poly.transform(X_train)
>```

>[!warning] 
>Se aumentiamo troppo il grado del polinomio possiamo arrivare a fare un **overfitting** sei dati 
# Logistic Regression

Vogliamo trasformare una *linear regression* che ritorna valori continui $[-\infty , \infty]$ in una funzione che ritorna un interallo compreso tra $[0,1]$ che verrà interpretato come la probabilità che un *istanza* appartenga ad una classe $0$ invece che ad una classe $1$  

Per fare questo utilizziamo una *sigmoide* ( *logistica* ) : 
$$P(y=1|x)=sig(z)=\frac{1}{1-e^{-z}}$$
dove $z$ è la *regressione lineare* : $z = X^T\upbeta + \epsilon$

## SVM