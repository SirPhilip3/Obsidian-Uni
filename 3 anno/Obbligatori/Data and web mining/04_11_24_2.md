---
publish: true
creation: 2024-11-26T11:01:00
---
# Text Processing

Come processiamo un testo 
## Text as a set of words

Dividiamo il testo in parole e le **normalizziamo** : 
+ trasformiamo tutte le parole in minuscolo 
>[!warning] 
>Altrimenti `STOP` e `stop` verrano considerate come parole differenti
+ eliminiamo la punteggiatura

Le **feature** saranno date dal *lessico* del dataset , ossia l'insieme di tutti i termini 
Ad ogni parola associeremo poi un valore bianario per indicarne la presenza all'interno di un determinato testo 
## Text as a bag of words

In questo caso , invece di considerare la presenza o no all'interno di un testo di una parola del *lessico* , consideriamo la frequenza di ogni parola 

>[!note] 
>Viene sempre eseguito il processo di **normalizzazione** come nel caso percedente
## TF-IDF ( term frequency–inverse document frequency )

Con questo metodo non tutte le parole sono le stesse , vogliamo identificare parole più generiche e darle un peso minore nella rappresentazione del testo 

**Definizioni**
+ *term frequency* o $tf(t)$ è il numero di volte che un termine ( $t$ ) compare in un testo 
+ **inverse document frequency** : approssima la specificità di un termine in una collezione di documenti
$$
idf(t) = \ln \frac{N_{docs}}{df(t)}
$$
Dove :
+ $N_{docs}$ è il numero di documenti della collezione
+ $df(t)$ è il numero di documenti che contengono il termine $t$

>[!note] 
>L'uso del logaritmo diminuisce l'inpatto di termini con alta frequenza 

Il peso finale dei termini sarà : $tf(t)\cdot idf(t)$
## Text normalization steps

Ulteriori step per *normalizzare* un testo ( oltre alla semplice riduzione in lettere minuscole e rimozione della punteggiatura ) :
+ **Stemming** : rimozione dei prefissi o suffissi da una parola 
>[!example] 
>`being` $\to$ `be` , `was`$\to$`was`
+ **Lemming** : ritorna la parola alla sua origine ( ossia ridurla al suo [Lemma](https://it.wikipedia.org/wiki/Lemma_(linguistica)) )
>[!example] 
>`being`$\to$`be` , `was`$\to$`be`
>>[!warning] 
>>`was` e `being` postrebbero significare qualcosa di diverso a seconda del contesto , il *lemming* rimuove questa differenza
# Naive Bayes

Il classificatore *Naive Bayes* utilizza la *probabilità condizionata* per determinare una **lable** da predire 
>[!example] 
>Qual'è la probabilità che un classe sia spam dato il suo testo ?
>

>[!note] 
>Questo classificatore usa i principi della [[Legge di Bayes]]

Nel nostro caso vogliamo rispondere alla seguente domanda :
	Cos'è $P(y|X)$ ? 

Dove : 
+ $y$ è la classe  
+ $X$ sono le **feature**

Oppure generalizzando per $m$ classi :
$$\arg{max}_{C_i} P(C_i|X)$$
Ossia la predizione finale sarà quella con *probabilità massima*

Per prendere il calcolo fattibile assumiamo che le **feature** siano *indipendenti* tra di loro , in questo modo il calcolo diventa :
$$P(X|C_i) = P(x_1|C_i)\cdot P(x_2|C_i)\cdot \ldots \cdot P(x_f|C_i)$$
E visto che ci interessa la classe che massimizza la probabilità e non la probabilità in sè possiamo togliere $P(X)$ 
$$\arg{max}_{C_i} P(C_i|X) = P(x_1|C_i)\cdot P(x_2|C_i)\cdot \ldots \cdot P(x_f|C_i) \cdot \frac{P(C_i)}{P(X)}$$
>[!note] 
>Possiamo precomputare $P(x_j|C_i)$ dal dataset attraverso due varianti del **Naive Bayes** :
>+ **Multimodal Naive Bayes** : 
>	Se le *feature* sono *categoriche* allora $P(x_j|C_i)$ è il numero di istanze della classe $C_i$ la cui feature $j$ ha gli stessi valori di $X$ diviso per la *cardinalità* di $C_i$ 
>+ **Gaussian Naive Bayes** : 
>	Se le *feature* sono *numeriche* allora assumiamo una [[Distribuzione normale ( Gaussiana )]] e dopo aver computato la *media* $\mu$ e la *deviazione standard* $\sigma$ limitate alle istanze di una classe $C_i$ avremo : 
>	$$P(x_j|C_i)=\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x_j-\mu)^2}{2 \sigma^2}}$$
## $P(x_j|C_i) = 0$

Se una probabilità va a $0$ tutto il prodotto va a $0$ , aggiungiamo quindi la **Laplace Correction** : 
Pretendiamo di osservare una istanza addizionale per ogni valore delle *feature* 
$$
P(x_j|C_i) = \frac{N_{ij}+1}{N_i+v}
$$
Dove :
+ $N_{ij}$ è il numero di istanze di classe $C_i$ con un valore della $j$-esima *feature* uguale a $x_j$
+ $N_i$ è il numero di istanze della classe $C_i$
+ $v$ è il numero totale  ddi valori unici della *feature* $j$ nel dataset $D$
## Missing Values

Se valori mancanti semplicemente saltiamo l'*istanza* a tempo di *training* 
A tempo di *test* assumiamo che $P(x_j | C_i)$ sia lo stesso per ogni *istanza* tolta 
## Computational Complexity

La computazione è molto veloce ma potremmo incontrare *numerical underflow* per via della moltiplicazione di numeri $0<x<1$ , per evitare ciò computiamo il logarimo :
$$
	\log P(C_i|X) = \log P(x_1|C_i) + \log P(x_2 | C_i) + \dots + \log P(x_f|C_i)
$$
## Detection error tradeoff (DET) curve

Questa curva illustra il trade-off tra *False Positives* e *False Negatives* , su scala normale 

![[Pasted image 20241127112431.png]]

>[!note] 
>+ **y** : $\frac{\# \text{False Negative}}{\# \text{Total Positives}}$ ( **False Negative Rate** )
>+ **x** : $\frac{\# \text{False Positives}}{\# \text{Total Negative}}$ ( **False Positive Rate** )

