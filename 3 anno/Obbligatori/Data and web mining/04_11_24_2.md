---
publish: true
creation: 2024-11-26T11:01:00
---
# Text Processing

Come processiamo un testo 
## Text as a set of words

Dividiamo il testo in parole e le **normalizziamo** : 
+ trasformiamo tutte le parole in minuscolo 
>[!warning] 
>Altrimenti `STOP` e `stop` verrano considerate come parole differenti
+ eliminiamo la punteggiatura

Le **feature** saranno date dal *lessico* del dataset , ossia l'insieme di tutti i termini 
Ad ogni parola associeremo poi un valore bianario per indicarne la presenza all'interno di un determinato testo 
## Text as a bag of words

In questo caso , invece di considerare la presenza o no all'interno di un testo di una parola del *lessico* , consideriamo la frequenza di ogni parola 

>[!note] 
>Viene sempre eseguito il processo di **normalizzazione** come nel caso percedente
## TF-IDF ( term frequency–inverse document frequency )

Con questo metodo non tutte le parole sono le stesse , vogliamo identificare parole più generiche e darle un peso minore nella rappresentazione del testo 

**Definizioni**
+ *term frequency* o $tf(t)$ è il numero di volte che un termine ( $t$ ) compare in un testo 
+ **inverse document frequency** : approssima la specificità di un termine in una collezione di documenti
$$
idf(t) = \ln \frac{N_{docs}}{df(t)}
$$
Dove :
+ $N_{docs}$ è il numero di documenti della collezione
+ $df(t)$ è il numero di documenti che contengono il termine $t$

>[!note] 
>L'uso del logaritmo diminuisce l'inpatto di termini con alta frequenza 

Il peso finale dei termini sarà : $tf(t)\cdot idf(t)$
## Text normalization steps

Ulteriori step per *normalizzare* un testo ( oltre alla semplice riduzione in lettere minuscole e rimozione della punteggiatura ) :
+ **Stemming** : rimozione dei prefissi o suffissi da una parola 
>[!example] 
>`being` $\to$ `be` , `was`$\to$`was`
+ **Lemming** : ritorna la parola alla sua origine ( ossia ridurla al suo [Lemma](https://it.wikipedia.org/wiki/Lemma_(linguistica)) )
>[!example] 
>`being`$\to$`be` , `was`$\to$`be`
>>[!warning] 
>>`was` e `being` postrebbero significare qualcosa di diverso a seconda del contesto , il *lemming* rimuove questa differenza
# Naive Bayes

## Detection error tradeoff (DET) curve