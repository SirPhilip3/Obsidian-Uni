---
creation: 2024-10-29T10:49:00
---
# Random Forest

L'algoritmo di *random forest* si basa sul **Bagging** 

La *random forest* cerca di migliorare l'algoritmo di **Bagging** , infatti questo lavora meglio quando gli alberi che costruisce sono differenti tra di loro , nel **Bagging** però la creazione degli alberi parte dai **bootrap samples** che , visto che possono condividere istanze tra di loro , non saranno indipendenti tra di loro 

>[!info] 
>Vogliamo cercare di creare alberi i più *dissimili* tra di loro 

Modifichiamo l'algoritmo di creazione degli alberi : 
+ **random input selection** : quando aggiungo un nodo la *feature* può essere scelta solo tra un insieme *casuale* piccolo ( generalmente la $\sqrt{n}$ dove $n$ è il numero iniziale di *feature* ) di *feature*

>[!note] 
>In questo caso , visto che il **bagging** non riduce il *bias* , creiamo degli alberi *fully grown* , in modo da avere degli alberi *accurati* ( *low bias* ) 

>[!note] 
>Visto che ad ogni nodo scegliamo un insieme casuale di *features* , un albero può quindi potenzialmente usare tutte le *feature* disponibili

 La *random forest* calcola la decisione finale come per il **bagging**

>[!summary] 
>Il *learning* è molto efficente visto che :
>+ Gli alberi possono essere allenati indipendentemente in parallelo
>+ Ad ogni nodo dobbiamo scegliere tra un insieme più piccolo di feature
>>[!note] 
>>Il calcolo della *root* e dei suoi due figli hanno il costo maggiore visto che l'algoritmo deve prendere in considerazione o tutto il dataset o metà

## Random Forest as a Similarity Estimator

