---
creation: 2024-10-29T10:49:00
publish: true
---
# Random Forest

L'algoritmo di *random forest* si basa sul **Bagging** 

La *random forest* cerca di migliorare l'algoritmo di **Bagging** , infatti questo lavora meglio quando gli alberi che costruisce sono differenti tra di loro , nel **Bagging** però la creazione degli alberi parte dai **bootrap samples** che , visto che possono condividere istanze tra di loro , non saranno indipendenti tra di loro 

>[!info] 
>Vogliamo cercare di creare alberi i più *dissimili* tra di loro 

Modifichiamo l'algoritmo di creazione degli alberi : 
+ **random input selection** : quando aggiungo un nodo la *feature* può essere scelta solo tra un insieme *casuale* piccolo ( generalmente la $\sqrt{n}$ dove $n$ è il numero iniziale di *feature* ) di *feature*

>[!note] 
>In questo caso , visto che il **bagging** non riduce il *bias* , creiamo degli alberi *fully grown* , in modo da avere degli alberi *accurati* ( *low bias* ) 

>[!note] 
>Visto che ad ogni nodo scegliamo un insieme casuale di *features* , un albero può quindi potenzialmente usare tutte le *feature* disponibili

 La *random forest* calcola la decisione finale come per il **bagging**

>[!summary] 
>Il *learning* è molto efficente visto che :
>+ Gli alberi possono essere allenati indipendentemente in parallelo
>+ Ad ogni nodo dobbiamo scegliere tra un insieme più piccolo di feature
>>[!note] 
>>Il calcolo della *root* e dei suoi due figli hanno il costo maggiore visto che l'algoritmo deve prendere in considerazione o tutto il dataset o metà

## Random Forest as a Similarity Estimator

Possiamo dire che due istanze sono simili se attraversano la *Random Forest* attraverso path simili , ossia due istanze sono simili se cadono nelle stesse foglie 

Possiamo quindi misurare la *similarità* di due istanze come la frazione di *foglie* comuni raggiunte da entrambe le istanze nell'attraversamento della *random forest* 

>[!example] 
>Se cadono nella stessa foglia per il $90\%$ degli alberi allora sono simili

In questo modo possiamo trovare **outliers** nel dataset 

Definiamo come *outlier* quelle istanze che sono molto *dissimili* da tutte le altre istanze del dataset , ossia definiamo come **outlier score** l'inverso della similarità comulativa tra tutte le altre istanze del dataset : 
$$out(o_i) = \frac{1}{\sum_{o_j \in D} RF\_sim(o_i , o_j)^2}$$
>[!warning] 
>Risulta essere molto costoso , visto che dobbiamo calcolare la similarità tra tutte le istanze per ogni istanza del dataset

## Random Forest for missing value imputation

Visto che con la *random forest* possiamo computare la *similarità* la sfruttiamo per riempire i *valori mancanti*

>[!info] Idea
>Riempiamo i *valori mancanti* con valori che derivano da istanze simili

>[!important] Algoritmo
>1. Rimpiazza i valori mancanti con la media delle istanze $j$ dove $f$ non è mancante , dove la media è pesata dalla *similarity score* di $(o_i,o_j)$ :
>$$o_i[f] = \frac{\sum_{o_j\in D} o_j[f]\cdot RF\_sim(o_i,o_j)}{\sum_{o_j\in D}RF\_sim(o_i,o_j)}$$
>>[!note] 
>>Istanze più vicine sono più importanti rispetto a quelle più lontane
>
>2. Allena una nuova *random forest* sui dati modificati e computa nuovi *similarity scores*
>3. Ripeti finchè non ci sono più miglioramenti 

