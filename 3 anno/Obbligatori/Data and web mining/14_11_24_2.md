---
creation: 2024-12-02T12:11:00
publish: true
---
## Computational complexity

Possiamo seguire tre strategie per scrivere l'*algoritmo* di 
### Naive Implementation

Questa avrà come complessità computazionale $O(n^3)$

Inizialmente dovremo computare tutta la tabella delle similarità : $O(n^2)$
Ogni step dell'algoritmo richiede : 
+ Trovare il minimo tra $n^2$ elementi , questo ha complessità $O(n^2)$
+ Compurare la distanza tra il nuovo cluster e gli altri $n$ cluster / punti $O(n)$  
+ Aggiornare $n$ entries nella *similarity matrix* $O(n)$

Visto che al massimo l'algoritmo può fare $n$ passi avremo che la complessità sarà : 
$$n^2+ n \cdot (n^2+n+n) = O(n^3)$$
### Min-Heap based

Questa avrà come complessità computazionale $O(n^2 \cdot \log n)$ 

Computa le $n^2$ similarità ( $O(n^2)$ ) 
Per ogni riga manteniamo un *min-heap* che mantiene le distanze verso gli altri $n$ punti $O(n\cdot n \log n)$
Ogni step dell'algoritmo richiede : 
+ Trovare il minimo tra i *min-heap* e rimuoverlo $O(n + \log n)$
+ Computare la distanza tra il nuovo cluster e gli altri $n$ *cluster* / *punti* $O(n)$
+ Inserire le nuove $n$ similarità nei vari *min-heap* $O(n\log n)$

La complessità totale sarà quindi : 
$$O(n^2) + O(n\cdot n \log n) = O(n^2 \log n)$$
### Single Linkage

Questa avrà come complessità computazionale $O(n^2)$
>[!note] 
Questo può avvenire poichè un nuovo *cluster* non può avvicinarsi rispetto ad uno più vecchio 

Computa le $n^2$ similarità $O(n^2)$
+ Per ogni *cluster* manteniamo solo il suo più vicino e la sua distanza in una lista di coppie $V(distance,id)$ di dimensione $n$ 
Ogni step dell'algortimo richiede : 
+ Trovare la distanza minima all'interno di $V$ : $O(n)$
+ Computare la matrice delle distanze tra il nuovo cluster e gli altri $n$ cluster / punti $O(n)$
+ Aggiornare le similarità contenute in $V$ : $O(n)$

La complessità totale sarà quindi : 
$$O(n^2) + O(n\cdot n ) = O(n^2)$$
>[!example] 
>![[Pasted image 20241202133447.png]]

# Density-Based Clustering Methods

>[!important] Features
>+ Scoprire cluster di dimensione arbitraria
>+ Gestire il rumore
>+ Utilizza parametri di densità come condizione di terminazione

## DBSCAN ( Density-Based Clustering Based on Connected Regions with High Density )

>[!info] Idea
>+ Trova oggetti in regioni *dense* 
>+ Unisci *regioni dense* vicine tra loro in un unico *cluster*
>+ *Rimuovi* oggetti in zone poco dense ( questi rappresentano il *rumore* )

### Desity Region

**Core point**
	Un punto in un dataset $\mathcal{D}$ è detto **core point** se ci sono almeno $MinPts$ entro una distanza $\epsilon$

**Neighborhood** 
	Il *Neighborhood* di un punto $p$ è : $N_{\epsilon}(p)=\{q\in \mathcal{D} | dist(p,q)\le \epsilon \}$ 
	Secondo questa definizione $p$ è un **core point** se $|N_{\epsilon}(p)|\ge MinPts$ 
>[!note] 
>$MinPts$ e $p$ sono *parametri di tuning* che devono essere decisi a priori

>[!example] 
>![[Pasted image 20241203090307.jpg]]

### Assegnazione di un Punto ad un Cluster

Un punto $q$ è **directly density-reachable** da $p$ se : 
+ $p$ è un **core point**
+ $q \in N_{\epsilon}(p)$ ( $q$ è nel **neighborhood** di $p$ )

Un punto $q$ è **density-reachable** da $p$ se :
+ C'è una *catena* di punti $p_i, \dots , p_n$ con $p_1 = p$ e $p_n= q$ tale che $p_{i+1}$ è **directly density-reachable** da $p_i$ 

>[!example] 
>![[Pasted image 20241203091316.png]]

>[!warning] 
>La **density reachability** non è simmettrica 
>
>$q$ potrebbe non essere un **core point** ( quindi se partissimo da $q$ non includeremmo $p$ )

