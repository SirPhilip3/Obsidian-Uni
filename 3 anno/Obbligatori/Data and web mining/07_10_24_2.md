---
creation: 2024-10-19T21:41:00
---
# Model Overfitting

Possiamo osservare *model overfitting* quando l'*accuracy* del dataset di *test* diminusce all'aumentare della *complessità* dell'**albero di decisione**

Questo può essere dovuto a : 
1. Bassa qualità dei dati ( dovuta alla presenza di *noise* o mancanza di un campione rappresentativo )
2. Elevata complessità del modello 

>[!warning] 
Il modo in cui costruiamo e valutiamo il modello influisce sulla sua complessità e accuratezza
>>[!example]
>>
>>Consideriamo la seguente predizione : se il prezzo di uno stock aumenti o diminuisca nei prossimi 10 giorni 
>>
>>Facciamo una predizione randomica : $P(correct)=0.5$
>>
>>Ora se vogliamo avere l'80% di accuracy dovremo predire correttamente almeno 8 dei 10 giorni , avremo quindi che la probabilità di fare 8 predizioni corrette di seguito sarà :
>>$$P(\#correct \ge 8) = \frac{{10 \choose 8}+{10 \choose 9}+{10 \choose  10}}{2^{10}} \approx 0.05$$
>>
>>Ora prendiamo 50 predittori , ognuno fa 10 predizioni random di seguito , selezioneremo il predittore più accurato 
>>
>>La probabilità che almeno un predittore fa 8 o più predizioni corrette sarà : 
>>$$P(\#correct \geq 8 ) = 1-(1-0.05)^{50} \approx 0.94$$
>>
>
>Il modello potrebbe quindi risultare preciso nel *training* solo per via di scielte random , sul *test* non funzionerà

# Model Selection

Per scielgiere tra diversi modelli in modo da evitare la situazione precedente abbiamo due metodi : 
## Validation Set

Divisiamo il *training set* ulteriormente per ricavarci un *validation set* ( generalmente $20\%$ del *training set* )

Questo verrà utilizzato per stimare gli *iperparametri* migliori del modello ( ex : numero di foglie etcc )

```python
# suddivisione train e test
X_train_80, X_test, y_train_80, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
# suddivisione train in train e validation
X_train, X_valid, y_train, y_valid  = train_test_split(X_train_80, y_train_80,test_size=0.20, random_state=42)

accuracies = []

for max_leaves in range(2,20):
    # train sul training set
    dt = DecisionTreeClassifier(max_leaf_nodes=max_leaves)
    dt.fit(X_train,y_train)

    # compute Accuracy
    train_acc=accuracy_score(y_true=y_train,y_pred=dt.predict(X_train))
    valid_acc=accuracy_score(y_true=y_valid,y_pred=dt.predict(X_valid))

    accuracies += [ [valid_acc, max_leaves] ]

# sciegliamo le migliori foglie prendendo l'accuracy massima da quella caolcata con il validation set
best_accuracy, best_leaves = max(accuracies)

# ri fittiamo il modello su tutto il training set iniziale
dt = DecisionTreeClassifier(max_leaf_nodes=best_leaves)
dt.fit(X_train_80,y_train_80)

test_acc = accuracy_score(y_true=y_test, y_pred=dt.predict(X_test))
```

>[!note] 
>Non possiamo utilizzare il *test set* poichè questo viene utilizzato per stimare  l'accuratezza del modello su dati mai visti
## Pruning

Abbiamo due famiglie di metodi per fare *pruning* :
+ **Pre-pruning** : durante la costruzione dell'albero definiamo delle condizioni che lo fanno terminare ( ex massimo numero di foglie , profondità etcc )
+ **Post-pruning** : lasciamo generare completamente l'albero 