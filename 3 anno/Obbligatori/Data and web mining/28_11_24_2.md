---
publish: true
creation: 2024-12-05T09:25:00
---
# Keras

La libreria **[keras](https://keras.io/)** implementa la **ANN** nel seguente modo : 

![[Pasted image 20241205092112.png]]

Dove :
+ Ogni **layer** è composto da neuroni dello stesso tipo 
+ L'**optimizer** calcola gli aggiornamenti dei *weight*  
## Keras Neurons

In **Keras** il neurone viene definito dalla sua **activation function** $f$ , questa viene applicata alla somma tra :
+ Il **bias** del neurone ( $b$ imparato dalla rete )
+ La sommatoria dei **pesi** $w_i$ moltiplicati per la predizione del neurone precedente $x_i$ ( $\sum_{i} w_i \cdot x_i$ )

![[1neuron.png]]

Un esempio di *funzione di attivazione* è la **sigmoide** : 
$$\sigma(z)=\frac{1}{1+e^{-z}}$$ Questa darà in output un valore compreso tra $0$ e $1$ 

### Activation Function


## Keras Loss Function

In **Keras** il calcolo della *loss function* viene svolto su **batch** di dati invece che su una singola istanza del dataset , questo fa in modo che abbiamo un valore più indicativo dell'andamento della rete per quando verranno aggiornati i pesi dall'**optimizer** 

>[!note] 
>Il dataset viene diviso completamente in batch , quando abbiamo processato tutti i batch di un dataset abbiamo passato un'**epoca** ( *epoch* ) di training 

>[!warning] 
>L'*accuracy* non è una buona *loss function* poichè non ci dice di quanto abbiamo sbagliato la predizione

Una **Loss Function** è la **binary cross-entropy** ( usata per classificatori binari ) : 
$$
H = -\sum_{(x,y)\in \mathcal{D}} ( T(y=1|x)\cdot \log(P(y=1|x) + T(y=0|x)\cdot \log(P(y=0|x) )
$$
**Dove** : 
+ $T$ è la distribuzione di probabiltà reale  
+ $P$ è la distribuzione di probabilità predetta dalla **ANN**

In pratica questa calcola la distanza tra la distribuzione di pobabilità reale e quella predetta dalla **ANN** , visto che prendiamo il logaritmo della predizione più piccolo sarà il suo valore più vicina sarà la predizione al target reale 
## Keras Layers 

Possiamo avere diversi tipi di *layer* in una **ANN** ,  il più semplice è il *layer denso* , ossia ogni nodo del *layer* $i$ è collegato a tutti i nodi del *layer* $i+1$

## Keras optimizer

Ci sono vari *optimizers* , quello più utilizzato è [Adam](https://keras.io/search.html?query=adam) , questo si basa sul **Gradient Descent**
### Learning rate

Il **learning rate** rappresenta quanto consentiamo di fare aggiornamenti grandi ai pesi , per questo è un parametro dell'optimizer

Maggiore è il **learning rate** più veloce sarà il training ma sarà meno accurato , più piccolo raggiunge un risultato più accurato ma anche più lentamente 

>[!note] 
>Generalmente si vuole un piccolo *learning rate* ( dell'ordine dello $0.001$ ) e un più grande numero di *epoche*

### Weights learning by Gradient Descent

