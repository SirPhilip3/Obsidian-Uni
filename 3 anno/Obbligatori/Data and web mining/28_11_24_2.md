---
publish: true
creation: 2024-12-05T09:25:00
---
# Keras

La libreria **[keras](https://keras.io/)** implementa la **ANN** nel seguente modo : 

![[Pasted image 20241205092112.png]]

Dove :
+ Ogni **layer** è composto da neuroni dello stesso tipo 
+ L'**optimizer** calcola gli aggiornamenti dei *weight*  
## Keras Neurons

In **Keras** il neurone viene definito dalla sua **activation function** $f$ , questa viene applicata alla somma tra :
+ Il **bias** del neurone ( $b$ imparato dalla rete )
+ La sommatoria dei **pesi** $w_i$ moltiplicati per la predizione del neurone precedente $x_i$ ( $\sum_{i} w_i \cdot x_i$ )

![[1neuron.png]]

Un esempio di *funzione di attivazione* è la **sigmoide** : 
$$\sigma(z)=\frac{1}{1+e^{-z}}$$ Questa darà in output un valore compreso tra $0$ e $1$ 

### Activation Function

Ci sono diversi tipi di **funzioni di attivazione** :
+ **sigmoide** : ritorna la probabilità di apparteneza ad una classe *binaria* 
>[!warning] 
>Usata solo per classificazione binaria , potrebbe avere problemi di buffer underflow se diventa un numero troppo piccolo
+ **ReLU** : ritorna il $max(0,x)$ dove $x$ è $b+ \sum_{i} x_i\cdot w_i$
## Keras Loss Function

In **Keras** il calcolo della *loss function* viene svolto su **batch** di dati invece che su una singola istanza del dataset , questo fa in modo che abbiamo un valore più indicativo dell'andamento della rete per quando verranno aggiornati i pesi dall'**optimizer** 

>[!note] 
>Il dataset viene diviso completamente in batch , quando abbiamo processato tutti i batch di un dataset abbiamo passato un'**epoca** ( *epoch* ) di training 

>[!warning] 
>L'*accuracy* non è una buona *loss function* poichè non ci dice di quanto abbiamo sbagliato la predizione

Una **Loss Function** è la **binary cross-entropy** ( usata per classificatori binari ) : 
$$
H = -\sum_{(x,y)\in \mathcal{D}} ( T(y=1|x)\cdot \log(P(y=1|x) + T(y=0|x)\cdot \log(P(y=0|x) )
$$
**Dove** : 
+ $T$ è la distribuzione di probabiltà reale  
+ $P$ è la distribuzione di probabilità predetta dalla **ANN**

In pratica questa calcola la distanza tra la distribuzione di pobabilità reale e quella predetta dalla **ANN** , visto che prendiamo il logaritmo della predizione più piccolo sarà il suo valore più vicina sarà la predizione al target reale 
## Keras Layers 

Possiamo avere diversi tipi di *layer* in una **ANN** ,  il più semplice è il *layer denso* , ossia ogni nodo del *layer* $i$ è collegato a tutti i nodi del *layer* $i+1$

## Keras optimizer

Ci sono vari *optimizers* , quello più utilizzato è [Adam](https://keras.io/search.html?query=adam) , questo si basa sul **Gradient Descent**
### Learning rate

Il **learning rate** rappresenta quanto consentiamo di fare aggiornamenti grandi ai pesi , per questo è un parametro dell'optimizer

Maggiore è il **learning rate** più veloce sarà il training ma sarà meno accurato , più piccolo raggiunge un risultato più accurato ma anche più lentamente 

>[!note] 
>Generalmente si vuole un piccolo *learning rate* ( dell'ordine dello $0.001$ ) e un più grande numero di *epoche*

### Weights learning by Gradient Descent

Data la **loss function** $J$ vogliamo trovare i pesi $w$ che minimizzano l'errore , questo viene fatto da un processo iterativo secondo la seguente regola : 
$$w_{i}^{t+1} = w^t - \eta \frac{dJ}{dw_i^t}$$
Dove : 
+ $\eta$ : è il **learning rate**
+ $-\frac{dJ}{dw_i^t}$ : è la tangete della **loss function** , questa deve essere resa negativa poichè la derivata da la direzione della crescita della loss ma noi vogliamo minimizzarla 
+ $t$ rappresenta l'indice dell'iterazione a cui siamo arrivati ( il **batch** corrente )

Aggiorniamo quindi $w^{t+1}$ ( tutti i pesi ) con $w^t- \eta \frac{dJ}{dw_i^t}$ , continuiamo ad interare finchè il gradiente diventa $0$ 

>[!note] 
>$w^{t+1}$ è calcolato dopo aver misurato la **loss function** su un **batch** $t$ del training set 

>[!warning] 

L'implementazione di questo meccanismo è detta **[back propagation](https://www.ibm.com/think/topics/backpropagation)** ( questo perchè la differenza di errore tra i *layer* viene propagato all'indietro )

# Universal Approximation Theorem

>[!important] 
Si può dimostrare che una *neural network* con un solo **Hidden Layer** ( un *layer* che si trova tra l'*input layer* e l'*output layer* ) , contenente un numero sufficentemente grande di *neuroni* può approssimare una qualsiasi funzione continua

