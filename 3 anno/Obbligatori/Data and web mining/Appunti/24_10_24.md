---
creation: 2024-10-24T12:11:00
tags:
  - appunti
---
**Feature Subset selection**

dato un insieme di feature trovare quelle importanti 

approccio iniziale metto tutti i segnali , garbage in garbage out 
più feature ci sono più test di correlazione facciamo più abbiamo correlazioni spurie , 

non c'è un modo ez per trovare l'insieme migliore , proviamo tutti i sottoinsiemi possibili , impraticabile

embedded proccess , durante il learning fanno della feature selection es l'albero di decisione ad ogni nodo scieglie una feature , alcune feature potrebbero non essere mai usate 

analisi a priori delle fature

matrice di correlazione tra tutte le coppie di feature , dobbiamo trarne delle conclusioni , analisi della correlazione per trovare ridondandi = porteanno stessa info una delle due le porto via , se correlazione tra feture e target label se scorrelate la butto via però potrebbe essere predittiva per un subset di dati

wrapper approaches , l'unnica cosa che guardo è l'accuracy , se aggiungegno una feature aumenta allora è utile

voglio solo fare learning

seq fwd selection e backwards

sequential perchè iterativamente aggiungiamo all'insieme di feture una alla volta 

algo è greedy , mi fermo se trovo numero max di feature o ho performance accettabili 

$F*$ insieme di feature selezionate prima testo la bontà indipendentemete ogni feature , proiezione del dataset delle feture in F* più una feture , all'inizio ridotta 1 colonna , alleno un modello e misuro la sua accuracy , la feture con migliore acc metto in F* poi provo di nuovo 

costo maggiore è l'allenamento del modello 

backward -> parto da tutte le feature e poi ne tolgo una quella che mi da drop minore in accuracy

non è detto che protano a stesse feature , più safe strategia backward , è più facile trovare una feature che non migliora molto

applicabile a qualsiasi algo di training

**Embedded approaches**  : sfruttano la conoscenza che il modello prende , non è blackbox 

foreste di alberi -> ad ogni momento l'albero fa un scelta della migliore feature , quelle non usate posso toglierle , ex se una feature è usata sulla root è più importante 

usiamo le misure di impurity per decidere che feture togliere , le feature scelte sono le migliori per quel modello , accumulo il gain per ogni nodo e albero della foresta 

**Feature importance** , normalizzato tra 0 e 1

potrei buttare via feature meno usate -> se accuracy non cambia molto better

supponiamo di avere due feture uguale nel dataset , feature importance dipende dall'ordine in cui le prendo 50/50 -> dipende da come vengono prese dal modello , se prendo sempre la prima , se le prendo casualmente -> la loro importance sarà dimezzata -> potrebbe uscire dalle top k feature 

ex faccio backwards selection , se tolgo la prima delle due con importnce uguale , se ricalcolo il modello l'altra aquisirà importance e torna nelle top k 

**recursive feature elimination** , backwards selection guradando feature importance , in generale si elminanon più di una feature alla volta

ranking -> a  che turno le ha eliminatea

cross validation per il numero di feature a cui mi fermo 

---

object -> interpreta tutto come string 

tutto in formato numerico 

oltre a rimpiazzare con media aggiungiamo per val mancanti una colonna che dia se è stata generata o no , divisione train test prima del cambiamento dei valori

pipeline sia su trainign che su test 

modello accurato dove ho etichetta e dove non ho etichetta deve predire stessa etichetta di un'istanza simile oppure fai clustering e metti etichetta del cluster

semi-supervised learning

se voglio togliere le righe in y senza val -> tolgo solo nel train 
potrei anche fare sampling per avere y con nan nello stessa dim 

one hot encoding -> un valore sostituito con una colonna binaria una per ogni valore distinto 

rappresenta bene gli insiemi 

posso avere moltissime colonne per esempio città -> devo aggregare per provincia/regione , se ho va distinti poco e molto frequenti -> quelli più freq gli metto collone uniche le altre metto other

ci sono dei casi in cui feature è categorica ma semantica ordinale -> basso medio alto , una colonna con basso = 0 , medio = 0.5 , alto = 1 ( non necessariamente lineare , devono rispettare la prop della semantica )

se faccio selection -> o le mantengo tutte o le tolgo tutte 