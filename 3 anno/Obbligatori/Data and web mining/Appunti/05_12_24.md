---
creation: 2024-12-05T12:09:00
tags:
  - appunti
---
1d convolutions

a serie temporali -> filtri unidimensionali : se segnale cresce sempre
end to end learning -> gli do immagine e gli do l'output -> il mezzo fa il learning 

i filtri hanno un bias oltre che i "pixel" 

1d -> in input un segnale temporale , un filtro potrebbe esser che abbia 3 el -> con pesi 1/3 -> media mobile , si sposta di un el alla volta 

preparare la rete -> visto che voglio task di predizione -> seq di input e il giorno sucessivo da predire , devo creare un insieme di giorni e la predizione che voglio dopo

finsestra di 100 -> osservo i primi 100 e predico 101 , mi sposto via via di 1 el , non ha nulla a che fare con la convoluzione , time series di input per cui voglio la predizione

tra convoluzione e output generalmente c'è un layer denso 

bianco = più importanza 

top1 accuracy vs top5 accuracy -> benchmark standard 

overfitting e best practices

overfitting -> con reti grandi , grandi parametri , necessitiamo di molti dati per evitare overfitting

loss peggiora -> overfitting

image data generator -> trasforma l'input per normalizzare il dataset 
flow -> get sottoinsieme di training e val

causa overfitting : 
+ rete scorretta , troppo complessa per i dati,  dati troppo pochi per la rete che abbiamo , invece di imparare dei pattern memorizza el del training , con pochi dati la rete si specializza su pochi pixel 
+ missmatch completo tra training e validation -> training solo auto frontali e poi lo testiamo su auto visti dal posteriore , ciò che ho imparato non rappresenta il test 

data augmentation

genriamo delle immagini su cui allenarmi -> giro , ruoto etcc -> trasformazioni geometriche 
facile per le immagini , altri dati potrebbe non essere così immediato

range di trasformazione , prendo a caso dentro il range e applico , ogni volta che estraggo un training riapplico la trasformazione , potrebbe non vedere mai due volte la stessa immagine 

dropout

fare in modo che un layer non si affidi a pochi neuroni del layer precedente , a tempo di trainign una precentuale dei neuroni vengono spenti in modo casuale , allistanza di traingin dopo un'altra metà a caso diversa da quella di prima , il layer dopo imparerà a fare predizione bassandosi su diversi neuroni

tipicamente 10 20 50 

in parallelo con ensamble learning 

dropour tra layer densi tipicamente ma anche no 

a tempo di test la rete vede tutto 

dropout ostacola il training quindi devo fare più epoche 

transfre learning -> sfrutta una rete già inparata -> prendo i pesi e li riutilizzo 
non posso usare però la stessa rete ma architettura standard -> pipilne di conv e poi densi per predizione 

prendo rete esistente elimino gli ultimi strati densi e li sostituisco con nuovi e li rialleno su un nuovo dataset , i convolutivi imparano angoli / rette mi servono sia per macchine che per penne , gli ultimi layer si specializzano sulla classe finale quindi non vanno bene per il mio dataset

meno pesi da imparare e quindi posso usare un dataset minore

