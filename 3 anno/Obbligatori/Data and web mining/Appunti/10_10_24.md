---
creation: 2024-10-10T12:09:00
tags:
  - appunti
---
**Linear regression**

altro metodo per fare supervised learning

complessità dei modelli !**!**!

regressione linerare : individuare una tendenza (retta) che indichi il trend del prezzo btc
retta che minimizza MSE

modello con più features : 
$$y = b_0 +b_1x_1+b_2x_2+\dots+b_nx_n$$
se il lineare non è abbastanza -> potrei fittare prabola ..... -> aumentare espressività del modello 
più aumento il grado più sono inversioni di tendenza che posso modellare 

per sklearn
crea altre features e poi fittare una regressione lineare : 

$$y = b_0 + b_1x + b_2x^2 $$
regressione lineare dove ho 2 feature x e x^2 
costruisco le features 
trasforma il train in features che servon per fittare pol di grado n
```python
poly = PolynomialFeatures(2, include_bias=False) # 2 o n
poly.fit(X_train) # just computes the number of additional features
poly.transform(X_train) # crea la seconda colonna
```


se aumento il grado -> **overfitting** del modello train error scende ma test error esplode

modello molto complesso -> molto instabile 

errroe diverso : 
+ bias : quanto il mio mdello è in grado di fittare i dati -> vicino all'obbiettivo ( modello quadratico ma i dasti sono cubici -> high bias )
+ variance : training set diversi tra loro stesse predizioni low varaincev

btc , giorni è feature inutile per predire il valore di BTC ( dovrei prendere anche più giorni precedenti alla predizione )

```python
def prepare (y,w):
    XX = np.array( [ y[i:i+w,0] for i in range(len(y)-w) ] )
    YY = np.array( y[w:])
    return XX, YY

X_train, y_train = prepare(y, 4) # finestra di training di 4 giorni , devo avere 4 giorni per predire il prossimo giorno
X_train.shape, y_train.shape
```

sta facendo una media mobile , il peso è concentrato sugli ultimi due giorni

```
model coeff. [[-0.04427384 0.08287761 0.05678508 -0.08067734 -0.12657576 0.02950194 0.14888631 -0.16822617 0.20957348 0.8699157 ]]
```

**Logistic regression**

classificazione binaria 

usando comb lineare delle feature voglio dire se un puto in classe 1 o 2

da regressione lineare a classe -> regressione lineare valore continuo da -inf a +inf , voglio sapere un valore binario A o B , il valore binario possiamo trasformarlo in prob che mi dica prob che stia in una classe 

attraverso *sigmoide*

$z$ comb lineare di feature
fitta log del rapporto delle prob di essere in classe 0 o 1

decision boundary graduale

quante rette ci sono che dividono le due classi -> infinite -> qual'è la migliore , trovare la retta che lascia il maggior spazio possibili tra le due classe **margine** ( rischio di fare errori nel test )

problemi linearmente separabili > senza sovrapposizioni
se ho sovrapposizioni cerco di avere pochi punti che calssifico male -> peggior accuracy 

**SVM** cerca di trovare decision boundary che massimizza il margine

Possiamo tollerare che qualche istanza stia dalla parte scorretta della predizione , la somma di tutti quegli errori nella parte da minimizzare , $C$ coefficente che preferisce tra margine largo ed minori errori

punti più vicini al margine = support vector 

algoritmo costoso 

C alto cerca di fare errore minimo , basso massimizzo il margine

C migliore da testare

**problemi non linearmente separabili**

non abbastanza espressivo , esiste vesrsione non lineare dell'**svm** : intuizione : 

es dataset con una sola feature 

qq   xxxx  qq  -> non separabile linearmente
trasmormo l'input -> es con x^2 -> si traforma in parabola -> linearmente separabile

nello spazio originale non posso classificare , trsaformo a più alta dimensionalità e faccio stesso ragionamento su spazio diverso

più dataset è grande più è intrattabile 