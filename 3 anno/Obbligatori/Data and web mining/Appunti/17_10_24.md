---
creation: 2024-10-17T12:15:00
tags:
  - appunti
---
k-fold cross validation : il partizionamento 60t 20v 20t mi potrebbe portare ad avere un validation facile o difficle portandomi a sovrastimare o sottstimare il modello 

k = parametro -> ripetizione del processo di validazione e prendere un average delle accuracy

es k=5 partiziono dataset in 5 parti : 5 esperimenti :
+ ultimo 5 = validation
+ quarto quinto = validatio etccc

fatto da `cross_val_score` cv = fold

campionamento casuale si può materializzare uno sbilanciamento 

*stratified sampling* -> invece di scieglerle casualmetne tra tutti scielgo casualmente tra le due classi , il rapporto tra le classi rimmarà uguale al dataset originale , usato in cross validation su sklearn

`GridSearchCV` -> cross validatio + stratified sampling

analisi di accouracy al cambiamento dei parametri etcc 

**Ensamble Methods** 

metodi per migliorare accuracy di un classificatore , possiamo mettere assieme più classificatori in modo da avere migliore accuracy 

mettendo assieme *weak learners* soluzione ottimale

3 classificatori con errore del 33% , predico quello che predice la maggioranza dei miei classificatori , migliora la mia accuracy

più classificatori aggiungo più sono accurato , allenare tanti classificatori differenti ? non posso a meno che non ho una quantità di dati infinita poichè il mio dataset non ha eventi indipendenti e quindi il miglioramento è inferiore

se divido troppo la singola parte è troppo piccola per allenare un modello

Facciamo *sampling* con replacement -> creo un nuovo dataset campionando da quello originale -> stessa dimensione del dataset originale -> estraggo istanza casuale , la metto nel campionamento , la rimetto nel dataset iniziale ( può essere ricampionata )

voglio allenare $k$ modelli

ripeto k volte, alla predizione raccolgo le predizione e trovo maggioranza (se decizione) o media(se regressione)

albero di decisione può sempre arrivare a 100% -> non ha senso mischiarlo ad altri modelli

**bagging** non mi garantisce modelli sufficentemente diversi tra di loro , sbagliano allo stesso modo
*boosting* : focalizzati di più su istanze che calssifico male

istanze hanno un peso associato , pesco con più probabilità istanze con peso più alto , ossia su cui faccio più errori

ho anche peso su modelli >> modello che performa bene << modello che perfoma male -> peso calcolato dall'errore del modello 

preferisco la predizione del modello con maggior peso nella decisione finale 

se errore minore di 0.5 (peggio di random) viene rimosso 

se predico correttametne il peso verrà moltiplicato per e^-a -> abbasso peso delle istanze che predico correttamente , contrario per pesi scorretti , questi pesi utilizzati nel prossimo campionamento

le istanze che classificavo bene , se diventano classificate male ritornano nel training 

quando applichiamo bagging o boosting ->
se ho modello con basso bias -> fare boosting è inutile
bagging abbassa varianza ( quanto si dispedre dalla media )
boosting abbasa il bias ( quanto pred corretta è lontana dalla media )

più modello è potente più la sua varianza può essere alta 

modello poco espressivo = bias alto , varianza bassa tutti modelli simili
molto espressiovo = basso bias , ma alta varianza 

knn k-> infty -> bias alto perchè arrivo a predire il valore medio

aumentando training set abbasso sia bias che variance 

overfittig con tanta varianzae poco bias , underfitting il contrario 