---
creation: 2024-10-03T12:06:00
tags:
  - appunti
---
**information gain**

ogni nodo un predicato

arriviamo sempre a foglie pure

albero con una sola foglia -> predizione -> prediciamo etichetta più frequente

altri metodi per misurate il *gain* 

**ID3** : misura di purezza delle foglie , meglio foglie più pure 

+ *entropia* : misura di casualità più è alta più c'è rumore / errore nel dataset

minima se ho tutte istanze della stessa classe 
*information gain* -> errore come prima però utilizzando l'entropia

esempio : sul nootbook

**Gain ratio** : 

information gain favorisce split con tanti figli -> hanno poche istanze + prob di avere purezza , non necessariamente buon modo per splittare

alberi k-ari 

\+ tagli fai viene tolta un po di entropia
entropia delle partizioni diventa grande di più divisione c'è , denominatore ha il numero di figli

non senso in alberi non binari

**gini** : misura di dispersione , quanto certe istanze sono dispersein certe classi 

minimo -> tutte istanze in una sola classe 
penalizza se lo split distribuisce equemente , favorisce partizioni pure 

sklear non implementa infogain

2 folgie -> prendo 2 predizioni

paritzionamenti preferiti -> dove trovo foglia pura 
nfoglie >> nclassi 
usati come criteri di fermata

più i dati sono complicati più foglie bo bisgono

**depth massima** -> a che livello di folie mi fermo ,albero bilanciato

se uso criterio di stopping nell'algoritmo di hunt crea alberi sbilanciati a sinistra 

guardo il best plit tra i due e continuo da li 

tengo traccia per ogni fogli qual'è il guadagno che avrei splittandola , scielgo quella con gain migliore 

calcolo lo split iniziale , finchè coda non è vuota prendo quello che mi da massimo gain

calcolo il miglior split del sinistro , lo metto nella coda , calcolo il dx e metto gain nella coda -> preno max etcc

rimane greedy

*regressione* stessa cosa ma per valori continui , devo aggiungere una misura ragionevole di errore -> MSE 

cosa metto nelle mie folgie ??? 
quella che mi minimizza l'MSE -> media delle etichette
altrimenti l'algoritmo è identico

modello + grande != modello più accurato

model overfitting