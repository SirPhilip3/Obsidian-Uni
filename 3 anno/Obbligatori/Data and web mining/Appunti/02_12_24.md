---
tags:
  - appunti
creation: 2024-12-02T14:01:00
---
10 classi invece che bianrio :
non abbiamo salti tra layer non consec e tutti nodi collegati tra di loro
output della rete ? -> 1 nueoroen nin output layer per ogni classe 

ogni nuorre si scpecializza su una classe 

funzione di attivazione : ????

sigmoide su ogni nodo tante probabilità -> quello con prob maggiore prediciamo quello 

La somma dei segnali da ogni neurore in putput deve dare 1 -> normalizzazione 

**SoftMax** : attivazione non sta dentro un neurone ma sta fuori da tutti i neuroni -> ci serve la somma di tutti gli altri e^segnael / somma degli altri e 

esalata i valori alti 

considerazioni per la funzione : ottimizzazione , facilità a livello numerico 

categorical cross entropy -> versione categirica della binary cross entropy 

**Loss** : funzione di loss e come rappresento l'output desiderato 
mappare output che vogliamo nello stesso spazio vettoriale -> vettoriale 
in quesoto modo do feedback a tutti i neuroni 

one hot encoding come dist di prob 

input della rete , l'importtne che sia ange confrontabile tra le varie feature , nel caso delle immagini non serve fare grandi cose , normalizzo tra 0 e 1 e converto in float visto che tutte le op sono float + linearizzo

none = dimensione dell'input

se non riesco ad avvicinarmi a 100 sul training vuol dire che la rete è poco espressiva 

sensibile a traslazioni , rotazioni e scaling 
standardizza l'input o nuova rete neurale 
aumentare il dataset lì dove ho difficoltà 

validation set , nel fit do % di validation 

posso dare peso ad ogni classe , dico dove concentrarsi dove vogliamo diminuire l'errore , daaset sbilancaito : più peso su classi minoritarie -> inversamente proprozionale alla frequenza 

class weight dictionary che mappa indici delle classi e peso 

senza funzione di attivazione output del neurone è b + media pesata per regressione 

**Convulational Neural Network**

shape dell'input ha 3 dimensioni : x , y , c -> dove c sono i canali 1 per i grigi , 
2 layer conv2d e maxpooling2d
attivazione relu

relu -> massimo tra 0 e l'input -> facile da calcolare , derivate ez 0 e 1 

filtro convolutivo : 32 filtri ognuno grande 3x3, applicato all'input produce un'altra immagine : individuono dei pattern
pesi dentro i conv2d vengono imparati dalla rete 
32 filtri = nuova immagine con 32 canali immagine grande come prima : ogni canale indica preenza o meno di un certo pattern in ogni pixel dell'immagine ,l'immagine a 32 canali è comunque sensibile allo spostamento ma i segnali saranno gli stessi per ogni cifra -> robusto rispetto a traslazione 

filtri altri -> si attiva se vede un insieme di filtri convolutivi precedenti 

per visualizzare filtri : generi immaine che attivia massimamente per quell'immagine 

layer convoulutivi gerarchici -> 
resistenza a scala / rotazione -> filtro che impari contemporaneamente le trasformaizoni 

