---
tags:
  - appunti
creation: 2024-12-02T14:01:00
---
10 classi invece che bianrio :
non abbiamo salti tra layer non consec e tutti nodi collegati tra di loro
output della rete ? -> 1 nueoroen in output layer per ogni classe 

ogni nuorre si scpecializza su una classe 

funzione di attivazione : ????

sigmoide su ogni nodo tante probabilità -> quello con prob maggiore prediciamo quello 

La somma dei segnali da ogni neurore in putput deve dare 1 -> normalizzazione 

**SoftMax** : attivazione non sta dentro un neurone ma sta fuori da tutti i neuroni -> ci serve la somma di tutti gli altri e^segnael / somma degli altri e 

esalata i valori alti 

considerazioni per la funzione : ottimizzazione , facilità a livello numerico 

categorical cross entropy -> versione categirica della binary cross entropy 

**Loss** : funzione di loss e come rappresento l'output desiderato 
mappare output che vogliamo nello stesso spazio vettoriale -> vettoriale 
in quesoto modo do feedback a tutti i neuroni 

one hot encoding come dist di prob 

input della rete , l'importtne che sia ange confrontabile tra le varie feature , nel caso delle immagini non serve fare grandi cose , normalizzo tra 0 e 1 e converto in float visto che tutte le op sono float + linearizzo

none = dimensione dell'input

se non riesco ad avvicinarmi a 100 sul training vuol dire che la rete è poco espressiva 

sensibile a traslazioni , rotazioni e scaling 
standardizza l'input o nuova rete neurale 
aumentare il dataset lì dove ho difficoltà 

validation set , nel fit do % di validation 

posso dare peso ad ogni classe , dico dove concentrarsi dove vogliamo diminuire l'errore , daaset sbilancaito : più peso su classi minoritarie -> inversamente proprozionale alla frequenza 

class weight dictionary che mappa indici delle classi e peso 

senza funzione di attivazione output del neurone è b + media pesata per regressione 

**Convulational Neural Network**

shape dell'input ha 3 dimensioni : x , y , c -> dove c sono i canali 1 per i grigi , 
2 layer conv2d e maxpooling2d
attivazione relu

relu -> massimo tra 0 e l'input -> facile da calcolare , derivate ez 0 e 1 

filtro convolutivo : 32 filtri ognuno grande 3x3, applicato all'input produce un'altra immagine : individuono dei pattern
pesi dentro i conv2d vengono imparati dalla rete 
32 filtri = nuova immagine con 32 canali immagine grande come prima : ogni canale indica preenza o meno di un certo pattern in ogni pixel dell'immagine ,l'immagine a 32 canali è comunque sensibile allo spostamento ma i segnali saranno gli stessi per ogni cifra -> robusto rispetto a traslazione 

filtri altri -> si attiva se vede un insieme di filtri convolutivi precedenti 

per visualizzare filtri : generi immaine che attivia massimamente per quell'immagine 

layer convoulutivi gerarchici -> 
resistenza a scala / rotazione -> filtro che impari contemporaneamente le trasformaizoni 
quando filtro si sovrappone al match risposta alta -> altrimenti bassa -> molto ridondandte mi servono solo dove sono i massimi -> Pooling per il riassunto 

2x2 maxpooling -> finestra 2x2 e ritorno il massimo , stesso output per immaigni traslate -> finestra non slitta di 1 ma salta , 2x2 diminuisce di 2x volte l'immagine 

per la classificazione vera e propria -> in uscia da maxpooling -> 32 immagini 
appiattisco le immagini e poi le collego al layer sucessivo , flatten -> input con 1 sola dimensione -> collego con layer denso e poi layer finale 

anche average pooling 
pixel iniziali vengono riassunti da immagini sempre più piccolie fino a 64 numeri dei nodi densi 

filtro convolutivo meno nodi di fully connected , numero di pesi cresce molto più lentamente 
