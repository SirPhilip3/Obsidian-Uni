---
creation: 2024-11-28T12:13:00
tags:
  - appunti
---
ANN -> layer densi -> ogni nuorone collegato a tutti neuroni del layer precedente , uìoutput a tutti i neuroni del layer sucessivo

input layer -> rappresentate per le feature in input alla ANN

processo di training -> architettura definita a priori -> ciò che imparo sono i pesi

optimizer decide come aggioranre i pesi 

loss calcolata su batch di istanze in modo da avere indicazione migliore per aggioranmento della rete -> meglio avere batch più picolli rispetto a tutta la rete in modo da non dovere elaborare tutti i dati 

batch dopo batch processo tutto il dataset -> 1 epoca di training

aggiorniamo i pesi a fine del batch ( non fatto in decision tree perchè molto performanti )

classificazione binaria -> prob di stare in classe gialla 
2 feature 4 istanze -> 1 solo neurone -> output sigmoide prob di stare in giallo 

devo trovare w1 w2 e b -> logistic regression

troppe epoche rischia overfitting -> 100% in training peggioro nel validation , controllo sul validation -> determina criterio di early stopping se sul validation set non migliori più 

loss function -> se ho predetto bene o male -> naturalmente accuracy > 0.5 classe 1 -> se real 1 altrimenti 0 , non buona loss non dice di quanto hai sbagliato 

binary cross entropy -> misura di distanza tra due dist di probabilità -> logatitmo della predizione -> più piccola più vicini siamo al target reale 

keras :

Dobbiamo dare input shape

```python
model.add( 
    layers.Dense(1,                    # no. of neurons
                 activation='sigmoid', # activation function
                 input_shape=(2,)      # shape of the input
                )
)
```

```python
model.compile( optimizer=optimizers.Adam(learning_rate=0.9), # optimizer
               loss='binary_crossentropy',       # loss function
               metrics=['acc'] )   # metrica , serve per  monitoring non ha impatto sul training , inpatto se usato per quando fermare numero di epoch
```

1 neurone = boundary lineare

ReLU activation function determinante nell'output meno per gli hidden layer , importante considerare la difficoltà di calcolare e non ha problemi numerici 

learning rate -> quanto consento di fare aggiornamenti forti ad ogni step di training 

in generale learing rate basso 

gradient descent -> per torvare valori dei parametri 
dato peso so la loss function , inizio con peso casuale , devo saper calcolare derivata prima della loss -> accuracy non va perchè va a scalini
tangente alla curva della loss -> ci dice direzione di crescita della loss , se vado in direzione opposta in zone dove la loss è più bassa -gradiente -> aggiornamento di quale sarà prossimo buono -> quello che avevo prima meno il gradiente , quando gradiente è zero mi fermo 

learning rate è quanto mi fido del gradiente g * l -> grande mi fido basso meno , valle potrebbe essere un minimo locale -> generalmente rete impara minimo locale con decent accuracy 
se avessi learning rate alto porei salatare tra valli 

rme per regressione -> loss function 
crossentropy loss -> per classificazione 

rete più piccola che da migliore accuracy in modo da non fare overfitting 

oltre a val set usiamo un set molto più piccolo dal val da ispezionare manualmente 

simulated anniling

si cambia learning rate durante il training -> all'inizio grande perchè partiamo da random , volgiamo andare verso una buona , learning rate moltiplicato per 0.95 ad ogni epoca , lo abbasso via via , adam -> learning rate adattivo -> diverso per ogni parametro -> cambiano a seconda 

discesa del gradiente o back propagation -> delta di errore pesato per i pesi ditreo verrà usato nella prossima epoca , derivata parziale 

universal approxximation theorem

1 solo hidden layer posso imparare qualsiasi cosa , espressività massima 
conviene dividerle in più hidden layer perchè il training converge prima 

più grandi prametri tante più istanze per porte imparare , diminuire nodi in modo da avere pochi pesi 

per la rete standardizzare / normalizzare input , se feature con range grande nasconde quella con range piccolo

deep nn -> 2+ hidden layer

inception module -> struttura gerachica con più output -> in modo che segnale della loss arrivi all'inizio più facilmente 

fc fully connected 
dato tot di parole predice la parola più probabile sucessiva 
