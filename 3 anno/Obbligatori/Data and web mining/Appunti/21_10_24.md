---
creation: 2024-10-21T13:51:00
tags:
  - appunti
---
**Random Forest**

base è il bagging 

applicazre bagging su modello semplice ma con alta varianza -> su alberi di decisione complesso -> con un albero completo (fino a foglie pure) fully grown trees

bagging -> difficoltà di creare modelli indipendenti tra di loro -> poichè utilizzo sempre lo stesso data set 

random forest -> modifica leggermente come creo l'albero per avere differenti alberi , random input selection -> quando aggiungo un nodo , la feature può essere scelta solo tra un insieme casuale piccolo di feature , n=radice del numero di feature

>[!note] 
possiamo aggiungere un peso a seconda di fino a dove viaggiano assieme , oppure qual'è la predizione finale dell'albero (se predizione è vicina) , meglio nel caso della regressione 

difficilmente creeranno alberi simili + diversità e poca correlazione 
>[!warning] 
>non fa parte della creazione del training , viene svolto per ogni foglia

albero nel suo complesso può toccare tutte le feature 

essendo bagging ogni albero è indipendente dagli altri , posso farlo anche in parallelo su più core 

ho meno feature da scegliere per foglia -> la costruzione degli alberi è più veloce 
>[!note] 
>root + i suoi figli hanno il costo maggiore visto che devo guardare l'intero D o metà

bootstrap sample -> stessa dim del dataset 

cosa usiamo decision tree, boosting, bagging, random forest -> generalmente boosting e random forest

MSE -> misura di errore -> cross_val_score > deve usare scoring in negativo in modo che il più alto ritorni

---

random forest possiamo utilizzaral per determinare quanto sonon simili due istanze

do in pasto a random forest e vedo se il loro comportamento è simile oppure no ( a seconda se i percorsi sono simili ) -> sono simili se cadono nella stessa foglia 

conto per quanti alberi della foresta cadono nella stessa foglia -> se cadono nella stessa foglia per il 90% degli alberi sono simili 

meglio del misurarlo nel boosting perchè ho un peso 

la distanza non necessariamente corrisponde alla decision boundary , vicinanza , chi è vicino rispetto alle distanze euclidee rimane vicino , quando cambio classe diventano subito distanti -> 100 foglia diversa 

random forest guarda anche l'etichetta 

la distanza calcolata può essere utilizzata per scovare outlier -> quando è lontano dalla distribuzione dei nostri punti 

outlier score -> 1/somma delle similarità tra tutti -> score maggiore > denominatore piccolo 

costo quadratico per calcolare tutti gli oggetti 

posso togliere i più outlier e fare learning sul rimanente 

**missing value imputation**

preprocessing -> media, moda

se ho valore mancante sostituisco con media intelligente -> media dei valori pesati , punti vicini = più importanti , punti lontani meno importanti 
