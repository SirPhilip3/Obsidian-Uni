---
creation: 2024-11-05T11:37:00
tags: []
---
# Accuracy Measures

Possiamo avere misure di precisione che si riferiscono alle singole classi : 

+ **Precision** 
Questa rappresenta quanto il modello è affidabile quando predice una certa classe $c$ 
$$\frac{\#\text{ di istanze correttamente classificate come}\ c}{\#\text{ di istanze predette come classe}\ c}$$
+ **Recall** 
Questo rappresenta il numero di volte che il modello ha predetto correttamente la classe $c$
$$\frac{\#\text{ di istanze correttamente classificate come}\ c}{\#\text{ di istanze di classe}\ c \text{ reali}}$$
+ **F-measure**
La *F-measure* è una *media armonica* di *Precision* e *Recall* 
$$\frac{2\times\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}$$
Un'altra versione è la *F-measure pesata* che aggiunge un peso per indicare a quale tra *Precision* e *Recall* si da più importanza
$$\frac{(1+\beta^2)\times\text{Precision}\times\text{Recall}}{\beta^2\times\text{Precision}+\text{Recall}}$$
>[!note] 
>Ciò significa che il *Recall* è considerato $\beta$ volte più importante della *Precision*

Generalmente vogliamo un numero unico per tutte le classi del modello in modo che ci dica la qualità del modello , per questo abbiamo due metodi per calcolare la *media* delle precedenti statistiche sulle varie classi :

**Macro** : Le statistiche vengono calcolate indipendentemente per ogni classe $c$ , la loro *media* verrà poi presa 

>[!note] 
>Classi con meno elementi contano di più

**Weighted** :  E' la stessa di *macro* ma pesata per la grandezza di ogni classe , in questo modo classi più popolose contano hanno più peso nella media finale

# Cost-Sensitive Learning

Associamo ad ogni tipo di missclassificazione che facciamo un costo , il modello ottimizzerà la somma pesata di questi errori

