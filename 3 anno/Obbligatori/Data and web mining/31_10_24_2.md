---
creation: 2024-11-05T11:37:00
publish: true
---
# Accuracy Measures

Possiamo avere misure di precisione che si riferiscono alle singole classi : 

+ **Precision** 
Questa rappresenta quanto il modello è affidabile quando predice una certa classe $c$ 
$$\frac{\#\text{ di istanze correttamente classificate come}\ c}{\#\text{ di istanze predette come classe}\ c}$$
+ **Recall** 
Questo rappresenta il numero di volte che il modello ha predetto correttamente la classe $c$
$$\frac{\#\text{ di istanze correttamente classificate come}\ c}{\#\text{ di istanze di classe}\ c \text{ reali}}$$
+ **F-measure**
La *F-measure* è una *media armonica* di *Precision* e *Recall* 
$$\frac{2\times\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}$$
Un'altra versione è la *F-measure pesata* che aggiunge un peso per indicare a quale tra *Precision* e *Recall* si da più importanza
$$\frac{(1+\beta^2)\times\text{Precision}\times\text{Recall}}{\beta^2\times\text{Precision}+\text{Recall}}$$
>[!note] 
>Ciò significa che il *Recall* è considerato $\beta$ volte più importante della *Precision*

Generalmente vogliamo un numero unico per tutte le classi del modello in modo che ci dica la qualità del modello , per questo abbiamo due metodi per calcolare la *media* delle precedenti statistiche sulle varie classi :

**Macro** : Le statistiche vengono calcolate indipendentemente per ogni classe $c$ , la loro *media* verrà poi presa 

>[!note] 
>Classi con meno elementi contano di più

**Weighted** :  E' la stessa di *macro* ma pesata per la grandezza di ogni classe , in questo modo classi più popolose contano hanno più peso nella media finale

>[!note] 
>Se vogliamo migliorare nelle classi rare diamo un costo maggiore agli errori svolti sulle classi rare 
# Cost-Sensitive Learning

Associamo ad ogni tipo di missclassificazione che facciamo un costo , il modello ottimizzerà la somma pesata di questi errori

>[!important] 
>`sklearn` ci permette solo di dare un peso alle classi , non disitinguendo i tipi di errori fatti dal modello , generalmente questi sono scelti come l'inverso della frequenza di quelle classi

# Binary Classifier

Un `DecisionTreeClassifier` ritorna oltre alla predizione anche la probabilità di appartenenza a quella classe ( determinata a seconda delle quantità di istanze delle due classi nella foglia )

>[!note] 
>Possiamo cambiare la soglia per cui distinguiamo tra le due classi per fare in modo di essere più certi di una predizione ( aumentando la *threshold* ) o meno certi diminuendola ( di default è `0.5` )

Per decidere come cambiare la *thershold* ( diventa un nuovo *iperparametro* ) dobbiamo introdurre delle nuove misure

Genereralmente per un modello binario possiamo identificare una *contingency table* , questa ci indica in quali casi ci troviamo in base alla predizione del modello e alla lable reale 

|                  |              |                 | Predicted Label |
| ---------------- | ------------ | --------------- | --------------- |
|                  |              | **Negative**    | **Positive**    |
| **Actual Label** | **Negative** | True Negatives  | False Positives |
|                  | **Positive** | False Negatives | True Positives  |
In base a questa tabella possiamo trovare le seguenti misure :

+ **True Negative Rate** : $\frac{\# \text{True Negatives}}{\# \text{Total Negatives}}$ 
	Vorremmo massimizzarlo nei casi in cui vogliamo essere certi di non dare dei *Falsi positivi*
+ **True Positive Rate** : $\frac{\# \text{True Positives}}{\# \text{Total Positives}}$
	Questa misura è la stessa del **recall** 
+ **False Positive Rate** : $\frac{\# \text{False Positives}}{\# \text{Total Negatives}}$

##  Receiver Operating Characteristic (ROC) curves

La curva **ROC** pre un dato modello mostra il trade-off tra il *True Positive Rate* ( *TPR* ) e il *False Positive Rate* ( *FPR* ) ( ossia è il plot del *true positive rate* contro il *false positive rate* )

Generalmente possiamo tollerare un maggiore *false positive rate* per avere un maggiore *true positive rate* ( **recall** )

**ROC** valuta il *TPR* e *FPR* per tutte le possibili *decision threshold* 

>[!note] 
>Con *threshold* uguale a $1$ il *FPR* va a $0$
>Con *threshold* uguale a $0$ il *TPR* va a $1$

>[!note] 
>L'area al di sotto la curva indica la qualità del modello su tutte le possibili *threashold*

