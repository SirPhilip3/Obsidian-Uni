---
creation: 2024-11-05T11:37:00
tags: []
---
# Accuracy Measures

Possiamo avere misure di precisione che si riferiscono alle singole classi : 

+ **Precision** 
Questa rappresenta quanto il modello è affidabile quando predice una certa classe $c$ 
$$\frac{\#\text{ di istanze correttamente classificate come}\ c}{\#\text{ di istanze predette come classe}\ c}$$
+ **Recall** 
Questo rappresenta il numero di volte che il modello ha predetto correttamente la classe $c$
$$\frac{\#\text{ di istanze correttamente classificate come}\ c}{\#\text{ di istanze di classe}\ c \text{ reali}}$$
+ **F-measure**
La *F-measure* è una *media armonica* di *Precision* e *Recall* 
$$\frac{2\times\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}$$
Un'altra versione è la *F-measure pesata* che aggiunge un peso per indicare a quale tra *Precision* e *Recall* si da più importanza
$$\frac{(1+\beta^2)\times\text{Precision}\times\text{Recall}}{\beta^2\times\text{Precision}+\text{Recall}}$$
>[!note] 
>Ciò significa che il *Recall* è considerato $\beta$ volte più importante della *Precision*

Generalmente vogliamo un numero unico per tutte le classi del modello in modo che ci dica la qualità del modello , per questo abbiamo due metodi per calcolare la *media* delle precedenti statistiche sulle varie classi :

**Macro** : Le statistiche vengono calcolate indipendentemente per ogni classe $c$ , la loro *media* verrà poi presa 

>[!note] 
>Classi con meno elementi contano di più

**Weighted** :  E' la stessa di *macro* ma pesata per la grandezza di ogni classe , in questo modo classi più popolose contano hanno più peso nella media finale

>[!note] 
>Se vogliamo migliorare nelle classi rare diamo un costo maggiore agli errori svolti sulle classi rare 
# Cost-Sensitive Learning

Associamo ad ogni tipo di missclassificazione che facciamo un costo , il modello ottimizzerà la somma pesata di questi errori

>[!important] 
>`sklearn` ci permette solo di dare un peso alle classi , non disitinguendo i tipi di errori fatti dal modello , generalmente questi sono scelti come l'inverso della frequenza di quelle classi

# Binary Classifier

Un `DecisionTreeClassifier` ritorna oltre alla predizione anche la probabilità di appartenenza a quella classe ( determinata a seconda delle quantità di istanze delle due classi nella foglia )

>[!note] 
>Possiamo cambiare la soglia per cui distinguiamo tra le due classi per fare in modo di essere più certi di una predizione ( aumentando la *threshold* ) o meno certi diminuendola ( di default è `0.5` )

Per decidere come cambiare la *thershold* ( diventa un nuovo *iperparametro* ) abbiamo le seguenti misure : 

|                  |              |                 | Predicted Label |
| ---------------- | ------------ | --------------- | --------------- |
|                  |              | **Negative**    | **Positive**    |
| **Actual Label** | **Negative** | True Negatives  | False Positives |
|                  | **Positive** | False Negatives | True Positives  |
