---
publish: true
creation: 2024-11-27T11:26:00
---
# Cluster Analysis

## Definizione

Un **cluster** è una collezione di oggetti che sono 
+ *simili* ad oggetti dello stesso cluster
+ *diversi* da oggetti di cluster diversi

La **cluster analysis** è utilizzata per trovare similarità tra oggetti e per raggruppare oggetti simili in un unico **cluster**

>[!note] 
>Questo è **unsupervised learning** , poichè non abbiamo i cluster all'interno del dataset 
## Cluster Quality

**Cluster** di alta qualità devono avere le seguenti caratteristiche : 
+ **high intra-class similarity** : le classi sono molto coese ( le istanze al suo interno sono molto simili )
+ **low inter-class similarity** : I vari *cluster* sono molto distinti tra loro  
## Considerazioni

Abbiamo due principali modi di *dividere* i dati :
+ **Single level**
+ **Hierarchical** ( generalmente si preferisce un partizionamento *multi-level hierachical* )

>[!example]
>![[Hierarchical_partitions.excalidraw]]

La *separazione* tra cluster può essere :
+ **Exclusive** : un'istanza appartiene ad una sola classe 
+ **Non-Exclusive** : un'istanza può appartenere a più istanze 

Possiamo fare due scelte per la *misura di similarità* : 
+ **Distance-based** ( distanza euclidiana ) : usata ad esempio in network stradali
+ **Connectivity-based** ( basata sulla densità  )

Spazio di clustering :
+ **Full space** : utilizziamo tutte le feature per determinare il clustering ( spesso usato quando i dati hanno poche dimensioni )
+ **Subspaces** : utilizziamo un sottoinsieme di feature ( usato in clustering multidimensionali )
## Approcci 

Abbiamo tre principali metodi per svolgere il *clustering* :
+ **Partitioning** : 
	+ Costruiscono varie partizioni che vengono poi valutate per qualche criterio ( ex [[MSE]] )
>[!note]- Implemenazioni 
>+ **k-means**
>+ *k-medoids*
>+ *CLARANS*

+ **Hierarchical** :
	+ Crea una decomposizione gerarchica dei dati usando qualche criterio
>[!note]- Implemenazioni
>+ *Diana*
>+ **Agglomerative Hierarchical Clustering**
>+ *BIRCH*
>+ *CAMELEON*

+ **Density-based** :
	+ Si basa su funzioni che determinano la connettività e densità delle istanze
>[!note]- Implementazioni
>+ **DBSCAN**
>+ *OPTICS*
>+ *DenClue*

### Altri Approcci 

#todo 
## K-means

### Misura di Qualità 

Come misura di qualità *k-means* cerca di minimizzare l'errore $E$ , rappresentato come la distanza tra punti all'iterno dello stesso cluster 

Per individuare il punto da cui calcolare la distanza per ogni cluster scegliamo $k$ punti *casuali*  

 L'errore $E$ ( anche detto [[SSE]] ( sum of squared errors ) ) da minimizzare sarà quindi : 
 $$
E = \sum_{i=1}^k \sum_{p\in C_i} dist(p,c_i)^2
$$
Dove : 
+ $k$ : sono il numero dei centri ( assumiamo che questo sia noto )
+ $C_i$ : rappresenta l'$i$-esimo cluster
+ $c_i$ : il centro del cluster $C_i$
+ $p$ : un punto dall'interno del cluster $C_i$

>[!note]
>Se usiamo la *distanza Euclidea* :
>$$E = \sum_{i=1}^k \sum_{p\in C_i} ||p-c_i||^2$$ 
### Algoritmo 

1. Presi $k$ centri a caso $c_i,\dots,c_k$ 
2. Dati i nuovi centri $c_i,\dots,c_k$ vogliamo trovare i cluster che minimizzano l'errore $E$ 
	1. Minimizziamo $||p-c_i||^2$ mettendo il punto $p$ nel cluster $C_i$ avente il centro $c_i$ più vicino
3. Dati i cluster $C_i,\dots,C_k$ troviamo l'insieme di centri che minimizzi l'errore $E$
	1. Il nuovo centro $c_i$ del cluster $C_i$ sarà la *media* dei punti $p\in C_i$
4. Ripeti da `2` fino a *convergenza* 

>[!note]- Derivazione dei centri
>$$
>\text{minimize}\quad E = \sum_{i=1}^k \sum_{p\in C_i} ||p-c_i||^2
>$$
>$$
>\begin{array}{rcl}
>\displaystyle\frac{\partial E}{\partial c_i} & = &  \displaystyle\frac{\partial}{\partial c_i} \sum\limits_{i=1}^{k} \sum\limits_{p\in C_i} (p-c_i)^2 = 0 \\
>&  & \sum\limits_{p\in C_i} 2 (p-c_i) (-1) = 0 \\ 
>&  & \sum\limits_{p\in C_i} p - \sum\limits_{p\in C_i} c_i = 0 \\ 
>&  & \sum\limits_{p\in C_i} p - |C_i|\cdot c_i = 0 \\ 
>&  & c_i = \displaystyle\frac{\sum\limits_{p\in C_i} p}{|C_i|} \\ 
>\end{array}
>$$

>[!note] Complessità Computazionale
>
>La complessità computazionale è : $O(tkn)$ 
>Dove : 
>+ $n$ è il numero degli oggetti
>+ $k$ è il numero di cluster
>+ $t$ è il numero di iterazioni 
>
>Tipicamente $k,t \llless n$ 

### Notes

**Pros** :
+ E' tra gli algoritmi più veloci
+ Buono per cluster *sferici* , *distribuiti* in modo simile e di *dimensioni simili*
+ Termina su un ottimo locale
**Cons** :
+ Dobbiamo specificare il numero di *cluster* $k$
+ Non adatto a scoprire *cluster* di forma non sferica 
+ Sensibile agli outliers ( un punto molto lontano influisce di più sul centro rispetto agli altri )
+ Applicabile solo a oggetti in uno spazio continuo $n$-dimensionale ( non può rappresentare dati categoriali )
+ Piccoli cluster potrebbero essere assorbiti da cluster più grandi

## K-means++

>[!note] 
>L'algoritmo dei **k-means** è sensibile alla scelta dei centroidi , potrebbero essere scelti all'interno degli stessi *cluster* 

Aggiunge una strategia per scegliere i centri dei cluster , poi semplicemente svogliamo **k-means** 

L'idea è quella di assicurarsi che i centroidi siano ben separati tra di loro in modo da coprire tutti i dati 
### Algoritmo

1. Scegliamo il primo centroide a caso dal dataset 
2. Ripeti per tutti i rimanenti centroidi :
	1. Per ogni punto $x$ nei dati computa la sua distanza dal *centroide* più vicino $d(x)$
	2. Assegna ad ogni punto probabilità proporzionale a $d(x)^2$ 
	3. Scegli un nuovo centroide a seconda delle proprietà precedenti 

### Elbow method

Vorremmo avere un numero di centri ragionevolmente basso , notiamo però che il Sum of Squares Error diminuisce aumentando i $k$ , per questo scegliamo un $k$ per cui il suo aumento porta ad una diminuzione marginale dell'*SSE*

>[!example] 
>![[Pasted image 20241201162927.png]]
>
>In questo caso ci fermiamo a $k=4$

>[!warning] 
>Si può dimostrare che dato $k$ trovare il *clustering* è un problema $NP-hard$ 

# Clustering Gerarchico

