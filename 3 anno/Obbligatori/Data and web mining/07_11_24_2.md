---
publish: true
creation: 2024-11-27T11:26:00
---
# Cluster Analysis

## Definizione

Un **cluster** è una collezione di oggetti che sono 
+ *simili* ad oggetti dello stesso cluster
+ *diversi* da oggetti di cluster diversi

La **cluster analysis** è utilizzata per trovare similarità tra oggetti e per raggruppare oggetti simili in un unico **cluster**

>[!note] 
>Questo è **unsupervised learning** , poichè non abbiamo i cluster all'interno del dataset 
## Cluster Quality

**Cluster** di alta qualità devono avere le seguenti caratteristiche : 
+ **high intra-class similarity** : le classi sono molto coese ( le istanze al suo interno sono molto simili )
+ **low inter-class similarity** : I vari *cluster* sono molto distinti tra loro  
## Considerazioni

Abbiamo due principali modi di *dividere* i dati :
+ **Single level**
+ **Hierarchical** ( generalmente si preferisce un partizionamento *multi-level hierachical* )

>[!example]
>![[Hierarchical_partitions.excalidraw]]

La *separazione* tra cluster può essere :
+ **Exclusive** : un'istanza appartiene ad una sola classe 
+ **Non-Exclusive** : un'istanza può appartenere a più istanze 

Possiamo fare due scelte per la *misura di similarità* : 
+ **Distance-based** ( distanza euclidiana ) : usata ad esempio in network stradali
+ **Connectivity-based** ( basata sulla densità  )

Spazio di clustering :
+ **Full space** : utilizziamo tutte le feature per determinare il clustering ( spesso usato quando i dati hanno poche dimensioni )
+ **Subspaces** : utilizziamo un sottoinsieme di feature ( usato in clustering multidimensionali )
## Approcci 

Abbiamo tre principali metodi per svolgere il *clustering* :
+ **Partitioning** : 
	+ Costruiscono varie partizioni che vengono poi valutate per qualche criterio ( ex [[MSE]] )
>[!note]- Implemenazioni 
>+ **k-means**
>+ *k-medoids*
>+ *CLARANS*

+ **Hierarchical** :
	+ Crea una decomposizione gerarchica dei dati usando qualche criterio
>[!note]- Implemenazioni
>+ *Diana*
>+ **Agglomerative Hierarchical Clustering**
>+ *BIRCH*
>+ *CAMELEON*

+ **Density-based** :
	+ Si basa su funzioni che determinano la connettività e densità delle istanze
>[!note]- Implementazioni
>+ **DBSCAN**
>+ *OPTICS*
>+ *DenClue*

### Altri Approcci 

#todo 
## K-means

### Misura di Qualità 

Come misura di qualità *k-means* cerca di minimizzare l'errore $E$ , rappresentato come la distanza tra punti all'iterno dello stesso cluster 

Per individuare il punto da cui calcolare la distanza per ogni cluster scegliamo $k$ punti *casuali*  

 L'errore $E$ ( anche detto [[SSE]] ( sum of squared errors ) ) da minimizzare sarà quindi : 
 $$
E = \sum_{i=1}^k \sum_{p\in C_i} dist(p,c_i)^2
$$
Dove : 
+ $k$ : sono il numero dei centri ( assumiamo che questo sia noto )
+ $C_i$ : rappresenta l'$i$-esimo cluster
+ $c_i$ : il centro del cluster $C_i$
+ $p$ : un punto dall'interno del cluster $C_i$

>[!note]
>Se usiamo la *distanza Euclidea* :
>$$E = \sum_{i=1}^k \sum_{p\in C_i} ||p-c_i||^2$$ 
### Algoritmo 

1. Presi $k$ centri a caso $c_i,\dots,c_k$ 
2. Dati i nuovi centri $c_i,\dots,c_k$ vogliamo trovare i cluster che minimizzano l'errore $E$ 
	1. Minimizziamo $||p-c_i||^2$ mettendo il punto $p$ nel cluster $C_i$ avente il centro $c_i$ più vicino
3. Dati i cluster $C_i,\dots,C_k$ troviamo l'insieme di centri che minimizzi l'errore $E$
	1. Il nuovo centro $c_i$ del cluster $C_i$ sarà la *media* dei punti $p\in C_i$
4. Ripeti da `2` fino a *convergenza* 

>[!note]- Derivazione dei centri
>$$
>\text{minimize}\quad E = \sum_{i=1}^k \sum_{p\in C_i} ||p-c_i||^2
>$$
>$$
>\begin{array}{rcl}
>\displaystyle\frac{\partial E}{\partial c_i} & = &  \displaystyle\frac{\partial}{\partial c_i} \sum\limits_{i=1}^{k} \sum\limits_{p\in C_i} (p-c_i)^2 = 0 \\
>&  & \sum\limits_{p\in C_i} 2 (p-c_i) (-1) = 0 \\ 
>&  & \sum\limits_{p\in C_i} p - \sum\limits_{p\in C_i} c_i = 0 \\ 
>&  & \sum\limits_{p\in C_i} p - |C_i|\cdot c_i = 0 \\ 
>&  & c_i = \displaystyle\frac{\sum\limits_{p\in C_i} p}{|C_i|} \\ 
>\end{array}
>$$

>[!note] Complessità Computazionale
>
>La complessità computazionale è : $O(tkn)$ 
>Dove : 
>+ $n$ è il numero degli oggetti
>+ $k$ è il numero di cluster
>+ $t$ è il numero di iterazioni 
>
>Tipicamente $k,t \llless n$ 

### Notes

**Pros** :
+ E' tra gli algoritmi più veloci
+ Buono per cluster *sferici* , *distribuiti* in modo simile e di *dimensioni simili*
+ Termina su un ottimo locale
**Cons** :
+ Dobbiamo specificare il numero di *cluster* $k$
+ Non adatto a scoprire *cluster* di forma non sferica 
+ Sensibile agli outliers ( un punto molto lontano influisce di più sul centro rispetto agli altri )
+ Applicabile solo a oggetti in uno spazio continuo $n$-dimensionale ( non può rappresentare dati categoriali )
+ Piccoli cluster potrebbero essere assorbiti da cluster più grandi

## K-means++

>[!note] 
>L'algoritmo dei **k-means** è sensibile alla scelta dei centroidi , potrebbero essere scelti all'interno degli stessi *cluster* 

Aggiunge una strategia per scegliere i centri dei cluster , poi semplicemente svogliamo **k-means** 

L'idea è quella di assicurarsi che i centroidi siano ben separati tra di loro in modo da coprire tutti i dati 
### Algoritmo

1. Scegliamo il primo centroide a caso dal dataset 
2. Ripeti per tutti i rimanenti centroidi :
	1. Per ogni punto $x$ nei dati computa la sua distanza dal *centroide* più vicino $d(x)$
	2. Assegna ad ogni punto probabilità proporzionale a $d(x)^2$ 
	3. Scegli un nuovo centroide a seconda delle proprietà precedenti 

### Elbow method

Vorremmo avere un numero di centri ragionevolmente basso , notiamo però che il Sum of Squares Error diminuisce aumentando i $k$ , per questo scegliamo un $k$ per cui il suo aumento porta ad una diminuzione marginale dell'*SSE*

>[!example] 
>![[Pasted image 20241201162927.png]]
>
>In questo caso ci fermiamo a $k=4$

>[!warning] 
>Si può dimostrare che dato $k$ trovare il *clustering* è un problema $NP-hard$ 

# Clustering Gerarchico

Utilizziamo la **distance / similarity matrix** come criterio di *clustering* 
+ La *matrice delle distanze* $D$ ha $D[i,j]$ la distanza tra l'oggetto $i$ e $j$
+ La *matrice di similarità* $S$ ha $S[i,j]$ la simialrità tra l'oggetto $i$ e $j$

>[!note] 
>Questo metodo non ha bisogno di conoscere a prescindere il numero $k$ di *cluster* 

Per trovare la gerachia di cluster possiamo avere due tecniche : 
+ **Agglomerativa** : 
	+ Inizialmente ogni oggetto appartiene al suo singolo cluster , sucessivamente verranno uniti iterativamente
>[!example] 
>![[Pasted image 20241202083808.jpg]]
+ **Divisiva** : 
	+ Inizialmetne tutti gli oggetti sono raggruppati in un singolo cluster e iterativamente viene diviso in cluster più piccoli 

## Hierarchical Agglomerative Clustering Algorithm 

```pseudo
	\begin{algorithm}
	\caption{HAC Algorithm}
	\begin{algorithmic}
	\State § Inizializza ogni punto come un cluster
	\For{$i = 1 \dots |\mathcal{D}|$}
		\State $C_i = \{o_i\}$
    \EndFor
    \State § Insieme di cluster
    \State $\mathcal{C} = \bigcup_i\{\mathcal{C_i}\}$
    \State § Loop di aggregazione
    \While{$|\mathcal{C}|>1$}
	    \State § Trova la coppia di cluster più vicina
	    \State $C_i,C_j = \arg\min_{C_x,C_y} D(C_x,C_y)$
	    \State § Rimpiazza $C_i$ e $C_j$ con il merge dei due
	    \State $C_{new} = C_i \cup C_j$
	    \State $\mathcal{C} = ((\mathcal{C} \backslash \{C_i\}) \backslash \{C_j\})\cup \{ C_{new}\}$
    \EndWhile
	\end{algorithmic}
	\end{algorithm}
```

Questo viene implementato attraverso una *matrice di distanza* $D[i,j]$ di dimensione $|\mathcal{D}|\times|\mathcal{D}|$ , questa sarà inizializzata a $D[i,j] = dist(o_i,o_j)$ ( ossia attraverso la distanza *Euclidiana* )

Quando mergiamo $C_i$ con $C_j$ ( $i<j$ ) :
+ La colonna e riga corrispondente al cluster $C_j$ vengono invalidate marcandole nel seguetne modo : $\forall k, D[j,k] = +\infty$ e $\forall k, D[k,j] = +\infty$
+ La colonna e riga corrispondente al cluster $C_i$ è usata per il nuovo cluster $C_{new}=C_i \cup C_j$ , settiamo quindi $\forall k , D[i,k] = sim(C_{new},C_k)$ e $\forall k , D[k,i] = sim(C_k,C_{new})$

>[!note] 
>Visto che generalmete le misure di similarità sono simmetriche ci basterebbe solo la matrice triangolare superiore o inferiore 

>[!important] 
>Gli algoritmi si differiscono per la misura di similarità tra cluster che usano 

>[!important] 
>I cluster sono anche determinati dai criteri di stopping : 
>+ Fermando il merge quando $k$ cluster vengono creati
>+ Uscendo dall'algoritmo quando i cluster sono tutti meno simili di una certa soglia determinata dall'utente

### Linkage Measures

Tipiche misure di similarità : 
+ **Single** : 
	+ $dist(C_i,C_j) = \min_{x\in C_i, y \in C_j} dist(x,y)$
	+ Ossia la distanza minima tra tutti gli oggetti nei due cluster 
>[!warning] 
>*Sovrastima* la similarità , potrebbe produrre *chaining* ( possiamo seguire forme arbitrarie )
>Basta che un elemento sia vicino per considerare i due cluster vicini 

>[!example]
>![[single-linkage.excalidraw]]

+ **Complete** :
	+ $dist(C_i,C_j) = \max_{x\in C_i, y \in C_j} dist(x,y)$
	+ Ossia la distanza massima tra tutti gli oggetti nei due cluster
>[!warning] 
>*Sottostima* la similarità , favorisce cluster globulari

>[!example] 
![[complete-linkage.excalidraw]]
+ **Average** :
	+ $dist(C_i,C_j) = \frac{1}{|C_i||C_j|} \sum_{x\in C_i, y \in C_j} dist(x,y)$
	+ Ossia prendiamo le distanze di tutti gli oggetti dei due cluster $C_i$ e $C_j$ e ritorniamo la loro media
>[!example] 
>![[average-linkage.excalidraw]]
+ **Centroid / Medoid** :
	+ $dist(C_i,C_j) = dist(o_i,o_j)$
	+ Ossia la distanza tra due cluster $C_i$ e $C_j$ è la distanza tra i loro *centroidi* 
+ **Ward** :
	+ Misuriamo l'aumento nell $SSE$ ( rispetto ai centroidi ) quando mergiamo due *cluster* 
>[!note] 
>**Ward** favorisce cluster più densi 

>[!example] 
![[Pasted image 20241202101038.png]]