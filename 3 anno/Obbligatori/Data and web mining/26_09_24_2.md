---
creation: 2024-09-27T15:41:00
publish: true
---
# Decision Trees

Questo modello di *machine learning* crea un **albero di decisione** che dividerà il dataset in input in modo da raffinare sempre di più la predizione della *label* 

Possiamo avere vari tipi di partizioni :
+ binaria
+ multiway split

>[!note] 
>Tutti gli alberi *multiway* possono essere divisi in una catena di *binary trees* 

Nel caso di valori continui abbiamo che in alberi binari decideremo se una certa *feature* supera una certa soglia 

>[!example] 
![[BinaryContinuosTree.excalidraw]]

Per decidere qual'è il migliore *split* si utilizza l'algorimo di **Hunt**

>[!important] Algoritmo di **Hunt**
>L'algoritmo di *Hunt* è un algoritmo ricorsivo , questo troverà il nodo padre migliore e sucessivamente deciderà il nodo migliore sinisto e destro

```pseudo
	\begin{algorithm}
	\caption{BuildTree(D)}
	\begin{algorithmic}
	\State BestSplit, BestGain = None
		\ForAll{feature $f$}
			\ForAll{threshold $t$}
			\State $Gain \leftarrow \text{goodness of the split} (f \le t)$
			\If{$Gain \ge BestGain$}
				\State $BestGain \leftarrow Gain$
				\State $BestSplit* \leftarrow ( f \le t )$
            \EndIf
            \EndFor
        \EndFor
    \If{BestGain = 0 or other stopping criteria is met}
	    \State $\mu \leftarrow \text{the best prediction for } D$
	    \Return Leaf($\mu$)
    \EndIf
    \State Let $f$ and $t$ be those of BestSplit = ($f \le t$)
    \State $D_L \leftarrow \{x \in D | x_f \le t\}$ (Left Partition)
    \State $L \leftarrow BuildTree(D_L)$ (Left Child)
    \State $D_R \leftarrow \{x \in D | x_f \gt t\}$ (Right Partition) 
    \State $R \leftarrow BuildTree(D_R)$ (Right Child)
	\end{algorithmic}
	\end{algorithm}
```

>[!summary] Explanation
>Dalle righe 1 - 7 troveremo il miglior nodo possibile provando per ogni *feature* ogni *threashold* 
>Alle righe 12 e 14 vengono assegnate come partizione sinistra le *feature* che sono $\le$ della *treshold* trovata mentre a destra quelle *feature* che hanno *threashold* $\gt$
>Le righe 13 e 14 rappresentano le chiamate ricorsive per costruire il sottoalbero sinistro e destro 
>Le righe 8 - 10 rappresentano le condizioni di uscita dalla ricorsione , infatti se non possiamo migliorare il *gain* o incontriamo altre condizioni di uscita ritoraniamo una *foglia* dell'albero ( una decizione )
>

**Come viene misurato il *Gain***

La misurazione del *gain* ha un grande effetto sull'algoritmo 

Un metodo è calcolare l'*accuracy* prima e dopo della scelta della *treshold* e *feature* , la differenza tra queste è il *gain*  

**Leaf Node**

Che predizione utilizziamo all'interno del *leaf* node ? 

Utilizzeremo la predizione che mi da il minimo *errore* ( calcolato come il complementare dell'*accuracy* )

#todo 

>[!note] 
>Quando non ho discriminato nulla la predizione è **randomica**

**Internal node**

Per i nodi interni il *gain* sarà dato dalla differenzza tra l'*errore* che farei inserendo il nodo ( prendiamo la media pesata tra i due errori destro e sinistro ) e l'errore che farei non inserendo il nodo :
$$Gain(f,t~|~{\cal D}) = Error({\cal D}) - \left(\frac{|{\cal D}_L|}{|{\cal D}|} Error({\cal D}_L) + \frac{|{\cal D}_R|}{|{\cal D}|} Error({\cal D}_R) \right)$$

>[!danger] 
>L'albero di decisione può arrivare anche ad una precisione del 100% , avendo un solo elemento nei nodi foglia , questo porta ad *overfitting* del modello ( faccio la decisione con il supporto di un solo datapoint )

>[!example] 
