---
creation: 2024-10-29T10:50:00
publish: true
---
# Feature subset selection

In molti scenari potremmo avere all'interno del dataset più *feature* del necessario , non tutte le *feature* potrebbero essere utilizzate da un dato modello

Vogliamo selezionare solo le *feature* più **informative** , ossia il numero minimo di *feature* necessarie per reggiungere la migliore *accuracy*

>[!note] 
>Ridurre il numero di *feature* **riduce** anche il rischio di avere *random correlation* ( [Examples](https://www.tylervigen.com/spurious-correlations) ) e **riduce** l'*errore di generalizzazione* del modello 

Abbiamo 3 differenti approcci per selezionare queste *feature* : 
+ **Embedded approaches** : Facciamo la selezione delle *feature* durante l'allenamento del modello 
+ **Filter approaches** : Seleziono le *feature* prima di allenare il modello , la scelta è arbitraria e può essere data da : ridondanza della feature , la feature non è correlata al *target*
+ **Wrapper Approaches** : Le *feature* vengono scelte in base alla loro contribuzione della *feature* all'*accuracy* complessiva del modello

## Wrapper Approaches

Due principali approcci *Wrapper* sono : 
+ **Sequential Forward Selection**
+ **Sequential Backward Selection**
### Sequential Forward Selection

L'algoritmo inizia con l'insieme di *feature* selezionate vuoto , ad ogni passo aggiungiamo ( in modo greedy ) quella *feature* che aumenta maggiormente l'*accuracy* del modello

```pseudo
	\begin{algorithm}
	\caption{Sequential Forward Selection}
	\begin{algorithmic}
	\State $F^* \leftarrow \emptyset$
	\Repeat
		\ForAll{$F_i \in (F \ F^*)$}
			\State $D_i \leftarrow D$ limitato alle feature in $(F^* \cup F_i)$
			\State Train $M_i$ on $D_i$
			\State $Q_i \leftarrow$ accuracy of $M_i$
	    \EndFor
	    \State $F^* = F^* \cup F_i$ in modo che $Q_i$ sia massimo
    \Until{Un criterio di stop viene incontrato}
	\Return $F^*$
	\end{algorithmic}
	\end{algorithm}
```
>[!note] 
>I criteri di stop sono definiti dall'utente , questi possono essere : 
>1. Abbiamo raggiunto il numero di *feature* che vogliamo
>2. Il miglioramento alla *accuracy* è marginale

>[!important] 
>L'algoritmo è *greedy* , infatti non produce la soluzione ottimale , questa necessiterebbe di considerare tutti i $2^d$ sottoinsiemi possibili delle *feature* ( $d$ è il numero di *feature* )
### Sequential Backward Selection

Funziona allo stesso modo della **Forward Selection** solo che l'insieme di *feature* di partenza è composto da tutte le *feature* presenti nel dataset , ad ogni iterazione dell'algoritmo toglieremo la *feature* che produce il più piccolo drop nell'*accuracy* del modello 

>[!warning] 
>I due metodi non portano allo stesso risultato , generalmente è meglio usare la **backward selection** poichè è più facile trovare un *feature* che non migliora molto l'*accuracy*

>[!note] 
>I metodi **Wrapper** possono essere utilizzati su qualsiasi modello poichè non necessitiamo di sapere il loro funzionamento
## Embedded Approaches based on Feature Importance

### Feature Importance

La *Feature Importnace* è una misura che il modello assegna ad ogni *feature* in base al loro contributo 

In una **Random Forest** questa viene calcolata accumulando il *gain* che quella feature ha su ogni nodo 

>[!note] 
>In *sklear* viene computata in base alla riduzione dell'impurità ( *gini* ) che viene poi normalizzata tra 0 e 1

Per scegliere le migliori *feature* per quel modello testiamo l'*accuracy* per le top $k$ *feature* più importanti ( con valori di *feature importance* alti ) , per ogni $k$

>[!warning] 
>La *fature importance* viene calcolata sul *training set* , la selezione potrebbe quindi non generalizzare per il *test set*

### Recursive Elimination 

>[!warning] 
>Se ci sono *feature* che sono diependenti le una dalle altre , allora la loro *feature importance* potrebbe essere divisa a metà tra le due , e a seconda di quale seleziono prima l'altra potrebbe uscire dalle top $k$ *feature* anche se ricalcolando l'*accuracy* potrebbe tornare all'interno delle top $k$

Usiamo **Backward Selection** eliminando ad ogni iterazione la *feature* con *importance* minore fino a raggiungere le top $k$ *features* 

```pseudo
	\begin{algorithm}
	\caption{Sequential Forward Selection}
	\begin{algorithmic}
	\State $F^* \leftarrow F$
	\Repeat
		\State $D^* \leftarrow D$ limitato alle feature in $F^*$
		\State Train $M$ on $D^*$
		\State $F^- \leftarrow$ la feature con score minore di $M$
		\State $F^* = F^*\ \backslash\ F^-$
    \Until{Un criterio di stop viene incontrato}
	\Return $F^*$
	\end{algorithmic}
	\end{algorithm}
```

>[!note] 
>Possiamo eliminare più di una *feature* ad ogni iterazione 

>[!note] 
>Usiamo **cross-validation** per selezionare il migliore subset di feature

# Feature Engineering

>[!important] 
>Prima di fare qulasiasi modifica al *dataset* , questo deve essere diviso in *train* e *test*

+ **Feature categoriche binarie** : 
	Mappiamo le categori in 0 e 1
+ **Feature categoriche k-arie** : 
	Utilizziamo *one-hot-encoding* , ossia per ogni categoria viene creata una nuova colonna con valori binari per indicare se tale istanza appartiene a quella categoria 

>[!example] One-hot-encoding
>
| Color  |       | Red | Yellow | Green |
| :----: | :---: | :-: | :----: | :---: |
|  Red   |       |  1  |   0    |   0   |
|  Red   |       |  1  |   0    |   0   |
| Green  | $\to$ |  0  |   0    |   1   |
| Yellow |       |  0  |   1    |   0   |
| Green  |       |  0  |   0    |   1   |
| Yellow |       |  0  |   1    |   0   |

+ **Feature con valori unici** : 
	Possono essere eliminate visto che non aumentano l'espressività del modello
+ **Valori mancanti** : 
	+ *Rimpiazzarli* con la *media* se la *feature* è numerica
	+ *Rimpiazzarli* con la *moda* ( maggiore frequenza ) se la *feature* è categorica
	+ Aggiungiamo una *feature* binaria che indica se il valore è mancante o no

