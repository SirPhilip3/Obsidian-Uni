---
creation: 2024-10-29T10:50:00
---
# Feature subset selection

In molti scenari potremmo avere all'interno del dataset più *feature* del necessario , non tutte le *feature* potrebbero essere utilizzate da un dato modello

Vogliamo selezionare solo le *feature* più **informative** , ossia il numero minimo di *feature* necessarie per reggiungere la migliore *accuracy*

>[!note] 
>Ridurre il numero di *feature* **riduce** anche il rischio di avere *random correlation* ( [Examples](https://www.tylervigen.com/spurious-correlations) ) e **riduce** l'*errore di generalizzazione* del modello 

Abbiamo 3 differenti approcci per selezionare queste *feature* : 
+ **Embedded approaches** : Facciamo la selezione delle *feature* durante l'allenamento del modello 
+ **Filter approaches** : Seleziono le *feature* prima di allenare il modello , la scelta è arbitraria e può essere data da : ridondanza della feature , la feature non è correlata al *target*
+ **Wrapper Approaches** : Le *feature* vengono scelte in base alla loro contribuzione della *feature* all'*accuracy* complessiva del modello

## Wrapper Approaches

Due principali approcci *Wrapper* sono : 
+ **Sequential Forward Selection**
+ **Sequential Backward Selection**
### Sequential Forward Selection

L'algoritmo inizia con l'insieme di *feature* selezionate vuoto , ad ogni passo aggiungiamo ( in modo greedy ) quella *feature* che aumenta maggiormente l'*accuracy* del modello

```pseudo
	\begin{algorithm}
	\caption{Sequential Forward Selection}
	\begin{algorithmic}
	\State $F^* \leftarrow \emptyset$
	\Repeat
		\ForAll{$F_i \in (F \ F^*)$}
			\State $D_i \leftarrow D$ limitato alle feature in $(F^* \cup F_i)$
			\State Train $M_i$ on $D_i$
			\State $Q_i \leftarrow$ accuracy of $M_i$
	    \EndFor
	    \State $F^* = F^* \cup F_i$ in modo che $Q_i$ sia massimo
    \Until{Un criterio di stop viene incontrato}
	\Return $F^*$
	\end{algorithmic}
	\end{algorithm}
```
>[!note] 
>I criteri di stop sono definiti dall'utente , questi possono essere : 
>1. Abbiamo raggiunto il numero di *feature* che vogliamo
>2. Il miglioramento alla *accuracy* è marginale

>[!important] 
>L'algoritmo è *greedy* , infatti non produce la soluzione ottimale , questa necessiterebbe di considerare tutti i $2^d$ sottoinsiemi possibili delle *feature* ( $d$ è il numero di *feature* )
### Sequential Backward Selection

Funziona allo stesso modo della **Forward Selection** solo che l'insieme di *feature* di partenza è composto da tutte le *feature* presenti nel dataset , ad ogni iterazione dell'algoritmo toglieremo la *feature* che produce il più piccolo drop nell'*accuracy* del modello 

>[!warning] 
>I due metodi non portano allo stesso risultato , generalmente è meglio usare la **backward selection** poichè è più facile trovare un *feature* che non migliora molto l'*accuracy*

>[!note] 
>I metodi **Wrapper** possono essere utilizzati su qualsiasi modello poichè non necessitiamo di sapere il loro funzionamento
## Embedded Approaches based on Feature Importance

### Feature Importance

La *Feature Importnace* è una misura che il modello assegna ad ogni *feature* in base al loro contributo 