---
creation: 2024-10-22T08:32:00
tags:
  - appunti
---
**Topic modeling**

Topic modeling -> BERTopic 

ampia mole di documenti -> modello ci deve dare -> clustering di documetni + desc dei contenuti

**BERTopic** : 

Embedding -> due metodi :
+ freqeuency based -> *bag of words* -> conta occorrenze delle parole , normalizzazione delle stringhe -> lowercase + rimuove punteggatura + togli stokwords
>[!note]  
>impossibile da standardizzare , gruppi di parole hanno significato diverso , si capisce dal contesto , in questo caso viene completamente perduto
+ embedding based -> text emebedding -> non basata sulle frequenze , allenare di llm su un corpus grande -> impara relazione tra parole 

le rappresenta internamente in spazio vettoriale , ogni parola vettore n dimensionale , ogni dim -> un contesto per parola , concetti più vicini = simili 

intelligenza artificiale su dei testi -> se impara info sbagliate prob

*SBERT* sentence *BERT* -> tutto il testo prima e dopo e deve indovinare cosa c'è nel mezzo , il contesto è imparato

lunghezza limitata ad un certo numero di token 

maledizione della dimensionalità -> con troppe dim è difficile capire la dist perchè tutti molto distanti -> riduzione della dimensionalità **UMAP** 

rappresentare come grafi , peso archi tra i vertici -> decadimento esponenziale -> punto più vicino peso 1 , i più lontani esp allo 0

combinare i grafi , somma i pesi e togliamo il loro prodotto degi archi

dobbiamo decidere quanti vicini ci sono -> più vicini prendo aumenta la qualità della rappresentazione 

HDBSCAN -> algoritmi di clustering , se non sappiamo cosa cerchiamo non c'è un unica verità 

centroid  
	centro del cluster come cosa più importante 
densità 
	quanti punti ci sono attorno 

clustering piatto -> un numero
gerarchicio -> tutti i numeri , dal più generale fino ad ogni singolo punto 1 cluster 

K-Means -> crea k centri , deve sapere qunati sono i cluster dentro il dataset

DBSCAN -> basato su densità , 2 parametri > mid dim dei cluseter , + distanza (per determinare densità) , punto denso = n (dim) el all'interno della distanza

HDBSCAN -> numero di punti minimo per definire un cluster

adesso usiamo bow nei cluster -> frequenza da sola non basta  -> si usa c-TF-IDF 

peso = freq del termine pesata per l'inveso della frequenza del termine su tutto il topic

lista di parole -> non capisce l'arogmento del cluster -> 
+ usiamo parole chiave
+ usare un'intelligenza artificiale 

descrizione > difficilmente rappresentabile 

**Model evaluation** 
+ dobbiamo capire se abbiamo sbagliato qualcosa

-> il modello può essere variato molto 

come sciegliere gli hyper parametri e valutarli ? 

emebedding -> grande è anche buono di solito ma lento
il modello deve essere allenato su testi simili ai nostri

performance sentence embedding -> maggiore è meglio

trustworthiness e continuity -> a seonda dell'emebedding > altrimenti non sono comparabili 
HBDSCAN -> relative validity -> cacola quanto sono ben separati i cluster tra di loro -> 1 best

topic coherence > si veder il risultato , si misura la coerenza del topic , rappresentano cosa c'è dentro al topic -> non tiene conto che stiamo valutando clustering a densità , high coheance non è molto discriminante per il modello 

https://github.com/MIND-Lab/OCTIS


